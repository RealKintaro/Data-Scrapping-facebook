{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abusif content classification","metadata":{}},{"cell_type":"code","source":"# import libraries \nimport pandas as pd\n# pd take screen width\npd.set_option('display.max_colwidth', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nimport string\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:06.263209Z","iopub.execute_input":"2023-01-08T10:13:06.263904Z","iopub.status.idle":"2023-01-08T10:13:06.271377Z","shell.execute_reply.started":"2023-01-08T10:13:06.263869Z","shell.execute_reply":"2023-01-08T10:13:06.270325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n\narabic_diacritics = re.compile(\"\"\"\n                             ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n\narabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\nenglish_punctuations = string.punctuation\npunctuations = arabic_punctuations + english_punctuations\n\n\ndef remove_urls (text):\n    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    return text\n\n\ndef remove_emails(text):\n    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n    return text\n\n# def remove_emoji(text):\n#     return emoji.get_emoji_regexp().sub(u'', text)\n\ndef remove_emoji(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\n#elongation\ndef normalization(text):\n    text = re.sub(\"[إأآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    text = re.sub(\"گ\", \"ك\", text)\n    return text\n\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', text)\n    return text\n\ndef remove_stopwords(text):\n    filtered_sentence = [w for w in text.split() if not w in arabic_stopwords]\n    return ' '.join(filtered_sentence)\n\n#cleaning function\ndef cleaning_content(line):\n    if (isinstance(line, float)):\n        return None\n    line.replace('\\n', ' ')\n    line = remove_emails(line)\n    line = remove_urls(line)\n    line = remove_emoji(line)\n    nline = [w if '@' not in w else 'USERID' for w in line.split()]\n    line = ' '.join(nline)\n    line = line.replace('RT', '').replace('<LF>', '').replace('<br />','').replace('&quot;', '').replace('<url>', '').replace('USERID', '')\n\n\n    # add spaces between punc,\n    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations}))\n\n    # then remove punc,\n    translator = str.maketrans('', '', punctuations)\n    line = line.translate(translator)\n\n    line = remove_stopwords(line)\n    line=remove_diacritics(normalization(line))\n    return line\n\ndef hasDigits(s):\n    return any( 48 <= ord(char) <= 57  or 1632 <= ord(char) <= 1641 for char in s)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:06.371451Z","iopub.execute_input":"2023-01-08T10:13:06.371810Z","iopub.status.idle":"2023-01-08T10:13:06.388823Z","shell.execute_reply.started":"2023-01-08T10:13:06.371780Z","shell.execute_reply":"2023-01-08T10:13:06.387769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the data\nannotated_data = pd.read_csv('/kaggle/input/offensive-dataset/final_manually_annotated.csv')\n#scrapped_data = pd.read_csv('/kaggle/input/offensive-content/final_dataset.csv', encoding='utf-16')\n\nannotated_data","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:06.391141Z","iopub.execute_input":"2023-01-08T10:13:06.391842Z","iopub.status.idle":"2023-01-08T10:13:06.427862Z","shell.execute_reply.started":"2023-01-08T10:13:06.391796Z","shell.execute_reply":"2023-01-08T10:13:06.426788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop nan values\nannotated_data.dropna(inplace=True)\nprint( f\" length of annotated_data : {len(annotated_data)}\")\nannotated_data['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:06.429509Z","iopub.execute_input":"2023-01-08T10:13:06.429888Z","iopub.status.idle":"2023-01-08T10:13:06.444085Z","shell.execute_reply.started":"2023-01-08T10:13:06.429853Z","shell.execute_reply":"2023-01-08T10:13:06.442028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select label racism\nabusif = annotated_data[annotated_data['label'] == 'Verbal abuse']\ndisplay(abusif)\n# delete label racism from df\nnot_abusif = annotated_data[annotated_data['label'] != 'Verbal abuse']\ndisplay(not_abusif)\nnot_abusif = not_abusif.iloc[:1136]\ndisplay(not_abusif)\n# concat abusif and df\ndata = pd.concat([not_abusif, abusif])\n# rename label diffrent than abusif to 'not abusif'\ndata['label'] = data['label'].apply(lambda x: 'not abusif' if x != 'Verbal abuse' else x)\ndata\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:06.448611Z","iopub.execute_input":"2023-01-08T10:13:06.449268Z","iopub.status.idle":"2023-01-08T10:13:06.491643Z","shell.execute_reply.started":"2023-01-08T10:13:06.449242Z","shell.execute_reply":"2023-01-08T10:13:06.490762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean the data\ndata['text'] = data['text'].apply(cleaning_content)\nprint(data['label'].value_counts())\ndata","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:06.494428Z","iopub.execute_input":"2023-01-08T10:13:06.496240Z","iopub.status.idle":"2023-01-08T10:13:06.710208Z","shell.execute_reply.started":"2023-01-08T10:13:06.496204Z","shell.execute_reply":"2023-01-08T10:13:06.709077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nlabels = data['label'].value_counts().keys()\nvalues = data['label'].value_counts()\n# Plot pie chart \nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:13:15.310875Z","iopub.execute_input":"2023-01-08T10:13:15.311587Z","iopub.status.idle":"2023-01-08T10:13:15.324381Z","shell.execute_reply.started":"2023-01-08T10:13:15.311550Z","shell.execute_reply":"2023-01-08T10:13:15.323403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tfidf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n# split the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n\n# vectorize the data\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)\n\n# train the model\nfrom sklearn.naive_bayes import MultinomialNB\n\nMNB_model = MultinomialNB().fit(X_train, y_train)\n\n# test the model\npredicted_mnb = MNB_model.predict(X_test)\n\n# evaluate the model\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, predicted_mnb))\nprobs_mnb = MNB_model.predict_proba(X_test)\n\n#confusion matrix\nplt.figure(figsize=(8,8))\nfx=sns.heatmap(metrics.confusion_matrix(y_test, predicted_mnb), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:03:38.385881Z","iopub.execute_input":"2023-01-08T10:03:38.387235Z","iopub.status.idle":"2023-01-08T10:03:38.727869Z","shell.execute_reply.started":"2023-01-08T10:03:38.387153Z","shell.execute_reply":"2023-01-08T10:03:38.726898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(probs_mnb)):\n    print(predicted_mnb[i], probs_mnb[i],y_test.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:03:38.730085Z","iopub.execute_input":"2023-01-08T10:03:38.731142Z","iopub.status.idle":"2023-01-08T10:03:38.791725Z","shell.execute_reply.started":"2023-01-08T10:03:38.731102Z","shell.execute_reply":"2023-01-08T10:03:38.789684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trying svm\nfrom sklearn import svm\nsvm_model = svm.SVC(kernel='linear', C=1, probability=True).fit(X_train, y_train)\npredicted_svm = svm_model.predict(X_test)\nprint(metrics.classification_report(y_test, predicted_svm))\n\n# print probabilities\nprobs_svm = svm_model.predict_proba(X_test)\n\n#confusion matrix\nplt.figure(figsize=(8,8))\nfx=sns.heatmap(metrics.confusion_matrix(y_test, predicted_svm), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()\n\nfor i in range(len(probs_svm)):\n    print(predicted_svm[i], probs_svm[i], y_test.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:03:38.795436Z","iopub.execute_input":"2023-01-08T10:03:38.796308Z","iopub.status.idle":"2023-01-08T10:03:41.489244Z","shell.execute_reply.started":"2023-01-08T10:03:38.796267Z","shell.execute_reply":"2023-01-08T10:03:41.487231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Initialize the classifier\nDTC = DecisionTreeClassifier().fit(X_train, y_train)\n\n# Make predictions on the test data\npredicted_DTC = DTC.predict(X_test)\n\nprint(metrics.classification_report(y_test, predicted_DTC))\n\n# print probabilities\nprobs_dtc = DTC.predict_proba(X_test)\n\n#confusion matrix\nplt.figure(figsize=(8,8))\nfx=sns.heatmap(metrics.confusion_matrix(y_test, predicted_DTC), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()\n\nfor i in range(len(probs_svm)):\n    print(predicted_DTC[i], probs_dtc[i], y_test.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:03:41.490849Z","iopub.execute_input":"2023-01-08T10:03:41.491456Z","iopub.status.idle":"2023-01-08T10:03:42.185406Z","shell.execute_reply.started":"2023-01-08T10:03:41.491416Z","shell.execute_reply":"2023-01-08T10:03:42.184292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Initialize the classifier\nRFC = RandomForestClassifier().fit(X_train, y_train)\npredicted_RFC = RFC.predict(X_test)\nprint(metrics.classification_report(y_test, predicted_RFC))\n# Make predictions on the test data\nprobs_rfc = RFC.predict_proba(X_test)\n\n#confusion matrix\nplt.figure(figsize=(8,8))\nfx=sns.heatmap(metrics.confusion_matrix(y_test, predicted_RFC), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()\n\nfor i in range(len(probs_svm)):\n    print(predicted_RFC[i], probs_rfc[i], y_test.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:03:42.187102Z","iopub.execute_input":"2023-01-08T10:03:42.187766Z","iopub.status.idle":"2023-01-08T10:03:43.885937Z","shell.execute_reply.started":"2023-01-08T10:03:42.187723Z","shell.execute_reply":"2023-01-08T10:03:43.884957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Initialize the classifier\nSVC = SVC(probability=True).fit(X_train, y_train)\npredicted_SVC = SVC.predict(X_test)\nprint(metrics.classification_report(y_test, predicted_SVC))\n# Make predictions on the test data\nprobs_svc = SVC.predict_proba(X_test)\n\n#confusion matrix\nplt.figure(figsize=(8,8))\nfx=sns.heatmap(metrics.confusion_matrix(y_test, predicted_SVC), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()\n\nfor i in range(len(probs_svm)):\n    print(predicted_SVC[i], probs_svc[i], y_test.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:03:43.887435Z","iopub.execute_input":"2023-01-08T10:03:43.888358Z","iopub.status.idle":"2023-01-08T10:03:46.585950Z","shell.execute_reply.started":"2023-01-08T10:03:43.888318Z","shell.execute_reply":"2023-01-08T10:03:46.584866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # balance the data\n# data = data.groupby('label').apply(lambda x: x.sample(1305, replace=True)).reset_index(drop=True)\n\n# data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:35:56.753306Z","iopub.execute_input":"2023-01-08T00:35:56.757754Z","iopub.status.idle":"2023-01-08T00:35:56.764208Z","shell.execute_reply.started":"2023-01-08T00:35:56.757714Z","shell.execute_reply":"2023-01-08T00:35:56.763253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nSTOPWORDS = set(stopwords.words('english'))\nfrom bs4 import BeautifulSoup\nimport plotly.graph_objs as go\nimport cufflinks\nfrom IPython.core.interactiveshell import InteractiveShell\nimport plotly.figure_factory as ff\nInteractiveShell.ast_node_interactivity = 'all'\nfrom plotly.offline import iplot\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:35:56.765767Z","iopub.execute_input":"2023-01-08T00:35:56.766485Z","iopub.status.idle":"2023-01-08T00:36:06.068551Z","shell.execute_reply.started":"2023-01-08T00:35:56.766450Z","shell.execute_reply":"2023-01-08T00:36:06.067607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:06.070383Z","iopub.execute_input":"2023-01-08T00:36:06.070654Z","iopub.status.idle":"2023-01-08T00:36:06.088002Z","shell.execute_reply.started":"2023-01-08T00:36:06.070629Z","shell.execute_reply":"2023-01-08T00:36:06.086933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 300\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(data['text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:06.089402Z","iopub.execute_input":"2023-01-08T00:36:06.090304Z","iopub.status.idle":"2023-01-08T00:36:06.172994Z","shell.execute_reply.started":"2023-01-08T00:36:06.090267Z","shell.execute_reply":"2023-01-08T00:36:06.171637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:06.174435Z","iopub.execute_input":"2023-01-08T00:36:06.174799Z","iopub.status.idle":"2023-01-08T00:36:06.233919Z","shell.execute_reply.started":"2023-01-08T00:36:06.174763Z","shell.execute_reply":"2023-01-08T00:36:06.232918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = pd.get_dummies(data['label']).values\nprint('Shape of label tensor:', Y.shape)\nY\n\n# encodedict = {'not abusif': 0, 'Verbal abuse': 1}\n# Y = np.array([encodedict[item] for item in data['label']])\n# print('Shape of label tensor:', Y.shape)\n# Y","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:06.235189Z","iopub.execute_input":"2023-01-08T00:36:06.235790Z","iopub.status.idle":"2023-01-08T00:36:06.246452Z","shell.execute_reply.started":"2023-01-08T00:36:06.235755Z","shell.execute_reply":"2023-01-08T00:36:06.245207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:06.248203Z","iopub.execute_input":"2023-01-08T00:36:06.248552Z","iopub.status.idle":"2023-01-08T00:36:06.258750Z","shell.execute_reply.started":"2023-01-08T00:36:06.248517Z","shell.execute_reply":"2023-01-08T00:36:06.257472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Sequential()\n# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n# model.add(SpatialDropout1D(0.2))\n# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dense(2, activation='sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# print(model.summary())\n\n# from keras.layers import BatchNormalization, GlobalMaxPooling1D\n# lstm = Sequential()\n# lstm.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n# lstm.add(BatchNormalization())\n# lstm.add(LSTM(100, return_sequences=True))\n# lstm.add(LSTM(100, return_sequences=True))\n# lstm.add(LSTM(100))\n# lstm.add(Dense(100, activation='relu'))\n# lstm.add(Dropout(0.5))\n# lstm.add(Dense(2, activation='sigmoid'))\n# lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# print(lstm.summary())\n\n# LSTM model\nlstm = Sequential()\nlstm.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nlstm.add(LSTM(256, input_shape = (len(word_index)+1, 300), return_sequences=True))\nlstm.add(Dropout(0.1))\nlstm.add(LSTM(128))\nlstm.add(Dense(16))\nlstm.add(Dense(2, activation = 'sigmoid'))\nlstm.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\nlstm.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:06.260432Z","iopub.execute_input":"2023-01-08T00:36:06.260873Z","iopub.status.idle":"2023-01-08T00:36:09.597304Z","shell.execute_reply.started":"2023-01-08T00:36:06.260838Z","shell.execute_reply":"2023-01-08T00:36:09.596363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 40\nbatch_size = 64\n\nhistory = lstm.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.25,callbacks=[EarlyStopping(monitor='val_loss', patience=25, min_delta=0.0001)])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:09.598804Z","iopub.execute_input":"2023-01-08T00:36:09.599170Z","iopub.status.idle":"2023-01-08T00:36:44.905820Z","shell.execute_reply.started":"2023-01-08T00:36:09.599136Z","shell.execute_reply":"2023-01-08T00:36:44.904880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape,Y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:44.907469Z","iopub.execute_input":"2023-01-08T00:36:44.908297Z","iopub.status.idle":"2023-01-08T00:36:44.915444Z","shell.execute_reply.started":"2023-01-08T00:36:44.908258Z","shell.execute_reply":"2023-01-08T00:36:44.914375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:44.916762Z","iopub.execute_input":"2023-01-08T00:36:44.917663Z","iopub.status.idle":"2023-01-08T00:36:53.634545Z","shell.execute_reply.started":"2023-01-08T00:36:44.917628Z","shell.execute_reply":"2023-01-08T00:36:53.633597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:53.636239Z","iopub.execute_input":"2023-01-08T00:36:53.636574Z","iopub.status.idle":"2023-01-08T00:36:53.827488Z","shell.execute_reply.started":"2023-01-08T00:36:53.636539Z","shell.execute_reply":"2023-01-08T00:36:53.826582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = lstm.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:53.828840Z","iopub.execute_input":"2023-01-08T00:36:53.829967Z","iopub.status.idle":"2023-01-08T00:36:54.139257Z","shell.execute_reply.started":"2023-01-08T00:36:53.829927Z","shell.execute_reply":"2023-01-08T00:36:54.138368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_lstm = lstm.predict(X_test)\n#print(predicted_lstm.shape)\npredicted_lstm","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:54.141731Z","iopub.execute_input":"2023-01-08T00:36:54.142071Z","iopub.status.idle":"2023-01-08T00:36:54.987804Z","shell.execute_reply.started":"2023-01-08T00:36:54.142043Z","shell.execute_reply":"2023-01-08T00:36:54.986718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #confusion matrix\n# plt.figure(figsize=(8,8))\n# fx=sns.heatmap(metrics.confusion_matrix(Y_test, predicted_lstm), annot=True, fmt=\"d\",cmap=\"GnBu\")\n# fx.set_title('Confusion Matrix \\n');\n# fx.set_xlabel('\\n Predicted Values\\n')\n# fx.set_ylabel('Actual Values\\n');\n# fx.xaxis.set_ticklabels(data['label'].unique())\n# fx.yaxis.set_ticklabels(data['label'].unique())\n# plt.show()\n\n# for i in range(len(probs_svm)):\n#     print(predicted_lstm[i],Y_test.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:54.993662Z","iopub.execute_input":"2023-01-08T00:36:54.993978Z","iopub.status.idle":"2023-01-08T00:36:54.998457Z","shell.execute_reply.started":"2023-01-08T00:36:54.993952Z","shell.execute_reply":"2023-01-08T00:36:54.997398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['Verbal abuse','not abusif']\npred_lstm=[]\nfor pred in predicted_lstm:\n    print(pred, labels[np.argmax(pred)])\n    pred_lstm.append(labels[np.argmax(pred)])\npred_lstm=np.array(pred_lstm)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:54.999990Z","iopub.execute_input":"2023-01-08T00:36:55.000635Z","iopub.status.idle":"2023-01-08T00:36:55.096790Z","shell.execute_reply.started":"2023-01-08T00:36:55.000598Z","shell.execute_reply":"2023-01-08T00:36:55.095740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_complaint = ['ولك يا حمار اسكت و ريحنا منك ولو ها المرة بس']\nseq = tokenizer.texts_to_sequences(new_complaint)\npadded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\npred = lstm.predict(padded)\nlabels = ['verbal abuse','none']\nprint(pred, labels[np.argmax(pred)])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T00:36:55.098868Z","iopub.execute_input":"2023-01-08T00:36:55.099720Z","iopub.status.idle":"2023-01-08T00:36:55.162513Z","shell.execute_reply.started":"2023-01-08T00:36:55.099666Z","shell.execute_reply":"2023-01-08T00:36:55.161585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Arabert","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing and Cleaning","metadata":{}},{"cell_type":"code","source":"def remove_hashtag(df, col = 'text'):\n    for letter in r'#.][!XR':\n        df[col] = df[col].astype(str).str.replace(letter,'', regex=True)\ndef normalize_arabic(text):\n    text = re.sub(\"[إأآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    text = re.sub(\"گ\", \"ك\", text)\n    return text\ndef remove_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1', text)\nfrom nltk.stem.isri import ISRIStemmer\nimport re\n\nstemmer = ISRIStemmer()\ndef processDocument(doc, stemmer): \n\n    #Replace @username with empty string\n    doc = re.sub(r'@[^\\s]+', ' ', doc)\n    doc = re.sub(r'_', ' ', doc)\n    doc = re.sub(r'\\n', ' ', doc)\n    doc = re.sub(r'\\r', ' ', doc)\n    doc = re.sub(r':///', ' ', doc)\n    doc = re.sub(r'///:', ' ', doc)\n    doc = re.sub(r'مستخدم@', ' ', doc)\n    doc = re.sub(r'[a-z,A-Z]', ' ', doc)\n    doc = re.sub(r'\\d', '', doc)\n    #Convert www.* or https?://* to \" \"\n    doc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',doc)\n    #Replace #word with word\n    doc = re.sub(r'#([^\\s]+)', r'\\1', doc)\n    # remove punctuations\n    # normalize the tweet\n#     doc= normalize_arabic(doc)\n    # remove repeated letters\n#     doc=remove_repeating_char(doc)\n    #stemming\n#     doc = stemmer.stem(doc)\n   \n    return doc\n\ndef deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:04:12.797858Z","iopub.execute_input":"2023-01-08T10:04:12.798924Z","iopub.status.idle":"2023-01-08T10:04:12.811938Z","shell.execute_reply.started":"2023-01-08T10:04:12.798877Z","shell.execute_reply":"2023-01-08T10:04:12.810960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyarabic.araby as araby\nframe=annotated_data\nframe=frame.drop_duplicates()\nremove_hashtag(frame)\nframe[\"text\"] = frame['text'].apply(lambda x: araby.strip_diacritics(x))\nframe[\"text\"] = frame['text'].apply(lambda x: normalize_arabic(x))\nframe[\"text\"] = frame['text'].apply(lambda x: processDocument(x, stemmer))\nframe[\"text\"] = frame['text'].apply(lambda x: deEmojify(x))\n\nframe","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:14:00.338645Z","iopub.execute_input":"2023-01-08T10:14:00.339063Z","iopub.status.idle":"2023-01-08T10:14:00.476098Z","shell.execute_reply.started":"2023-01-08T10:14:00.339025Z","shell.execute_reply":"2023-01-08T10:14:00.474990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select label racism\nabusif = frame[frame['label'] == 'Verbal abuse']\n# delete label racism from df\nnot_abusif = frame[frame['label'] != 'Verbal abuse']\nnot_abusif = not_abusif.iloc[:1136]\n# concat abusif and df\ndata = pd.concat([not_abusif, abusif])\n# rename label diffrent than abusif to 'not abusif'\ndata['label'] = data['label'].apply(lambda x: 'non abusif' if x != 'Verbal abuse' else x)\ndata\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:14:30.783088Z","iopub.execute_input":"2023-01-08T10:14:30.783490Z","iopub.status.idle":"2023-01-08T10:14:30.804666Z","shell.execute_reply.started":"2023-01-08T10:14:30.783452Z","shell.execute_reply":"2023-01-08T10:14:30.803674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:14:43.022132Z","iopub.execute_input":"2023-01-08T10:14:43.022497Z","iopub.status.idle":"2023-01-08T10:14:43.027677Z","shell.execute_reply.started":"2023-01-08T10:14:43.022467Z","shell.execute_reply":"2023-01-08T10:14:43.026379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:14:46.880396Z","iopub.execute_input":"2023-01-08T10:14:46.880816Z","iopub.status.idle":"2023-01-08T10:14:46.885973Z","shell.execute_reply.started":"2023-01-08T10:14:46.880782Z","shell.execute_reply":"2023-01-08T10:14:46.884903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\ntrain, val = train_test_split(data[['label','text']], test_size=0.2, random_state=42)\n\n\nlbl_enc = LabelEncoder()\ntrain.loc[:,\"label\"] = lbl_enc.fit_transform(train[\"label\"])\nval.loc[:,\"label\"] = lbl_enc.fit_transform(val[\"label\"])\njoblib.dump(lbl_enc,\"label_encoder.pkl\")\ntrain.to_csv(\"train.csv\",index=False)\nval.to_csv(\"val.csv\",index=False)\nlbl_enc.classes_","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:14:52.909786Z","iopub.execute_input":"2023-01-08T10:14:52.910545Z","iopub.status.idle":"2023-01-08T10:14:52.937350Z","shell.execute_reply.started":"2023-01-08T10:14:52.910508Z","shell.execute_reply":"2023-01-08T10:14:52.936270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['label'].value_counts(),val['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:14:58.433671Z","iopub.execute_input":"2023-01-08T10:14:58.434063Z","iopub.status.idle":"2023-01-08T10:14:58.443136Z","shell.execute_reply.started":"2023-01-08T10:14:58.434026Z","shell.execute_reply":"2023-01-08T10:14:58.441929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArabicDataset(Dataset):\n    def __init__(self,data,max_len,model_type=\"Mini\"):\n        super().__init__()\n        self.labels = data[\"label\"].values\n        self.texts = data[\"text\"].values\n        self.max_len = max_len\n        model = {\"Mini\": \"asafaya/bert-mini-arabic\",\n                \"Medium\": \"asafaya/bert-medium-arabic\",\n                \"Base\": \"asafaya/bert-base-arabic\",\n                \"Large\": \"asafaya/bert-large-arabic\"}\n        self.tokenizer = AutoTokenizer.from_pretrained(model[model_type])\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self,idx):\n        text = \" \".join(self.texts[idx].split())\n        label = self.labels[idx]\n        inputs = self.tokenizer(text,padding='max_length',\n                                max_length=self.max_len,truncation=True,return_tensors=\"pt\")\n        #input_ids,token_type_ids,attention_mask\n        return {\n            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n                      \"token_type_ids\":inputs[\"token_type_ids\"][0],\n                      \"attention_mask\":inputs[\"attention_mask\"][0],\n                     },\n            \"labels\": torch.tensor(label,dtype=torch.long) \n        }","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:15:06.729333Z","iopub.execute_input":"2023-01-08T10:15:06.730017Z","iopub.status.idle":"2023-01-08T10:15:06.738967Z","shell.execute_reply.started":"2023-01-08T10:15:06.729966Z","shell.execute_reply":"2023-01-08T10:15:06.737669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArabicBertModel(pl.LightningModule):\n    def __init__(self,model_type=\"Mini\"):\n        super().__init__()\n        model = {\"Mini\": (\"asafaya/bert-mini-arabic\",256),\n                \"Medium\": (\"asafaya/bert-medium-arabic\",512),\n                \"Base\": (\"asafaya/bert-base-arabic\",768),\n                \"Large\": (\"asafaya/bert-large-arabic\",1024)}\n        self.bert_model = AutoModel.from_pretrained(model[model_type][0])\n        self.fc = nn.Linear(model[model_type][1],18)\n    \n    def forward(self,inputs):\n        out = self.bert_model(**inputs)#inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])\n        pooler = out[1]\n        out = self.fc(pooler)\n        return out\n    \n    def configure_optimizers(self):\n        return optim.AdamW(self.parameters(), lr=0.0001)\n    \n    def criterion(self,output,target):\n        return nn.CrossEntropyLoss()(output,target)\n    \n    #TODO: adding metrics\n    def training_step(self,batch,batch_idx):\n        x,y = batch[\"inputs\"],batch[\"labels\"]\n        out = self(x)\n        loss = self.criterion(out,y)\n        return loss\n    \n    def validation_step(self,batch,batch_idx):\n        x,y = batch[\"inputs\"],batch[\"labels\"]\n        out = self(x)\n        loss = self.criterion(out,y)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:15:10.722550Z","iopub.execute_input":"2023-01-08T10:15:10.722941Z","iopub.status.idle":"2023-01-08T10:15:10.734694Z","shell.execute_reply.started":"2023-01-08T10:15:10.722906Z","shell.execute_reply":"2023-01-08T10:15:10.733562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArabicDataModule(pl.LightningDataModule):\n    def __init__(self,train_path,val_path,batch_size=12,max_len=100,model_type=\"Mini\"):\n        super().__init__()\n        self.train_path,self.val_path= train_path,val_path\n        self.batch_size = batch_size\n        self.max_len = max_len\n        self.model_type = model_type\n    \n    def setup(self,stage=None):\n        train = pd.read_csv(self.train_path)\n        val = pd.read_csv(self.val_path)\n        self.train_dataset = ArabicDataset(data=train,max_len=self.max_len,model_type=self.model_type)\n        self.val_dataset = ArabicDataset(data=val,max_len=self.max_len,model_type=self.model_type)\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True,num_workers=4)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False,num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:15:15.363648Z","iopub.execute_input":"2023-01-08T10:15:15.364050Z","iopub.status.idle":"2023-01-08T10:15:15.372706Z","shell.execute_reply.started":"2023-01-08T10:15:15.363996Z","shell.execute_reply":"2023-01-08T10:15:15.371566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### callbacks","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn,optim\n\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:15:18.939725Z","iopub.execute_input":"2023-01-08T10:15:18.940429Z","iopub.status.idle":"2023-01-08T10:15:18.945545Z","shell.execute_reply.started":"2023-01-08T10:15:18.940393Z","shell.execute_reply":"2023-01-08T10:15:18.944551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: getting different models sizes results\nMODEL_TYPE = \"Medium\"\ndm = ArabicDataModule(train_path=\"./train.csv\",\n                val_path = \"./val.csv\",\n                batch_size=128,max_len=60 , model_type=MODEL_TYPE)\n\nmodel = ArabicBertModel(model_type=MODEL_TYPE)\n#trainer = pl.Trainer(accelerator='gpu', devices=1,max_epochs=20, default_root_dir='.')\ntrainer = pl.Trainer(accelerator='gpu',max_epochs=40, devices=1, default_root_dir='.')\ntrainer.fit(model,dm)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:15:46.444925Z","iopub.execute_input":"2023-01-08T10:15:46.445587Z","iopub.status.idle":"2023-01-08T10:19:59.617662Z","shell.execute_reply.started":"2023-01-08T10:15:46.445549Z","shell.execute_reply":"2023-01-08T10:19:59.616577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\npreds = []\nreal_values = []\n\nload = ArabicDataModule(train_path=\"./train.csv\",\n                           val_path = \"./val.csv\",\n                batch_size=512,max_len=60)\nload.setup()\ntrain_dataloader = load.train_dataloader()\n\nprogress_bar = tqdm(range(len(train_dataloader)))\n\nprint(model.eval())\nfor batch in train_dataloader:    \n    x,y = batch[\"inputs\"],batch[\"labels\"]\n    inp = {k: v.to(device) for k, v in x.items()}\n    \n    with torch.no_grad():\n        outputs = model(inp)\n\n    predictions = torch.argmax(outputs, dim=-1)\n    \n    preds.extend(predictions)\n    real_values.extend(y)\n\n    progress_bar.update()\n    \npreds = torch.stack(preds).cpu()\nreal_values = torch.stack(real_values).cpu()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:21:14.672984Z","iopub.execute_input":"2023-01-08T10:21:14.674137Z","iopub.status.idle":"2023-01-08T10:21:25.646751Z","shell.execute_reply.started":"2023-01-08T10:21:14.674084Z","shell.execute_reply":"2023-01-08T10:21:25.645527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(real_values, preds, target_names=lbl_enc.classes_))","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:24:00.503102Z","iopub.execute_input":"2023-01-08T10:24:00.504260Z","iopub.status.idle":"2023-01-08T10:24:00.519644Z","shell.execute_reply.started":"2023-01-08T10:24:00.504202Z","shell.execute_reply":"2023-01-08T10:24:00.518559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics as metrics\n\nplt.figure(figsize=(16,10))\nfx=sns.heatmap(metrics.confusion_matrix(real_values, preds), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:24:04.237046Z","iopub.execute_input":"2023-01-08T10:24:04.237767Z","iopub.status.idle":"2023-01-08T10:24:04.511944Z","shell.execute_reply.started":"2023-01-08T10:24:04.237729Z","shell.execute_reply":"2023-01-08T10:24:04.510935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\npreds = []\nreal_values = []\n\nload = ArabicDataModule(train_path=\"./train.csv\",\n                           val_path = \"./val.csv\",\n                batch_size=512,max_len=60)\nload.setup()\ntest_dataloader = load.val_dataloader()\n\nprogress_bar = tqdm(range(len(test_dataloader)))\n\nmodel.eval()\nfor batch in test_dataloader:    \n    x,y = batch[\"inputs\"],batch[\"labels\"]\n    inp = {k: v.to(device) for k, v in x.items()}\n    \n    with torch.no_grad():\n        outputs = model(inp)\n\n    predictions = torch.argmax(outputs, dim=-1)\n    \n    preds.extend(predictions)\n    real_values.extend(y)\n\n    progress_bar.update()\n    \npreds = torch.stack(preds).cpu()\nreal_values = torch.stack(real_values).cpu()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:24:10.189699Z","iopub.execute_input":"2023-01-08T10:24:10.190098Z","iopub.status.idle":"2023-01-08T10:24:20.471767Z","shell.execute_reply.started":"2023-01-08T10:24:10.190064Z","shell.execute_reply":"2023-01-08T10:24:20.470579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(real_values, preds, target_names=lbl_enc.classes_))","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:24:22.909459Z","iopub.execute_input":"2023-01-08T10:24:22.909857Z","iopub.status.idle":"2023-01-08T10:24:22.925061Z","shell.execute_reply.started":"2023-01-08T10:24:22.909823Z","shell.execute_reply":"2023-01-08T10:24:22.923264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics as metrics\n\nplt.figure(figsize=(16,10))\nfx=sns.heatmap(metrics.confusion_matrix(real_values, preds), annot=True, fmt=\"d\",cmap=\"GnBu\")\nfx.set_title('Confusion Matrix \\n');\nfx.set_xlabel('\\n Predicted Values\\n')\nfx.set_ylabel('Actual Values\\n');\nfx.xaxis.set_ticklabels(data['label'].unique())\nfx.yaxis.set_ticklabels(data['label'].unique())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:24:26.349204Z","iopub.execute_input":"2023-01-08T10:24:26.349576Z","iopub.status.idle":"2023-01-08T10:24:26.612186Z","shell.execute_reply.started":"2023-01-08T10:24:26.349544Z","shell.execute_reply":"2023-01-08T10:24:26.611199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to a new file with a .pt extension\ntorch.save(model, 'abusif_classifier.pt')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:24:54.514419Z","iopub.execute_input":"2023-01-08T10:24:54.514791Z","iopub.status.idle":"2023-01-08T10:24:55.644855Z","shell.execute_reply.started":"2023-01-08T10:24:54.514759Z","shell.execute_reply":"2023-01-08T10:24:55.643794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Save the model\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# Load the model\nwith open('model.pkl', 'rb') as f:\n    model = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T10:08:54.923502Z","iopub.execute_input":"2023-01-08T10:08:54.925330Z","iopub.status.idle":"2023-01-08T10:08:56.814813Z","shell.execute_reply.started":"2023-01-08T10:08:54.925281Z","shell.execute_reply":"2023-01-08T10:08:56.813788Z"},"trusted":true},"execution_count":null,"outputs":[]}]}