{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## In this Notebook I will start with the very Basics of RNN's and Build all the way to latest deep learning architectures to solve NLP problems. It will cover the Following:\n\n### Simple RNN's\n### Word Embeddings : Definition and How to get them\n### LSTM's\n### GRU's\n### BI-Directional RNN's\n### Encoder-Decoder Models (Seq2Seq Models)\n### Attention Models\n### Transformers - Attention is all you need\n### BERT","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:04:44.801457Z","iopub.execute_input":"2023-01-07T20:04:44.801849Z","iopub.status.idle":"2023-01-07T20:04:50.604949Z","shell.execute_reply.started":"2023-01-07T20:04:44.801761Z","shell.execute_reply":"2023-01-07T20:04:50.604019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import our dataset \ntrain = pd.read_csv('/kaggle/input/augmented-ds/augmented_tweets2.csv', encoding='utf-16')\ntest = pd.read_csv('/kaggle/input/test-df/test_tweets.csv', encoding='utf-16')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:07:16.821386Z","iopub.execute_input":"2023-01-07T20:07:16.821729Z","iopub.status.idle":"2023-01-07T20:07:16.880321Z","shell.execute_reply.started":"2023-01-07T20:07:16.821698Z","shell.execute_reply":"2023-01-07T20:07:16.879296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will do binary classification and we will take sample dataset:\ntrain.drop(['Unnamed: 0','Unnamed: 0.1','id',], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:08:05.140639Z","iopub.execute_input":"2023-01-07T20:08:05.140980Z","iopub.status.idle":"2023-01-07T20:08:05.150608Z","shell.execute_reply.started":"2023-01-07T20:08:05.140950Z","shell.execute_reply":"2023-01-07T20:08:05.149722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:33:06.863150Z","iopub.execute_input":"2023-01-07T20:33:06.863535Z","iopub.status.idle":"2023-01-07T20:33:06.868685Z","shell.execute_reply.started":"2023-01-07T20:33:06.863502Z","shell.execute_reply":"2023-01-07T20:33:06.867741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:08:10.033994Z","iopub.execute_input":"2023-01-07T20:08:10.034345Z","iopub.status.idle":"2023-01-07T20:08:10.045299Z","shell.execute_reply.started":"2023-01-07T20:08:10.034312Z","shell.execute_reply":"2023-01-07T20:08:10.044192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop_duplicates(subset=\"tweet\",\n                     keep=False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:08:58.679194Z","iopub.execute_input":"2023-01-07T20:08:58.679595Z","iopub.status.idle":"2023-01-07T20:08:58.697567Z","shell.execute_reply.started":"2023-01-07T20:08:58.679564Z","shell.execute_reply":"2023-01-07T20:08:58.696628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting max number of words coz this will help us in padding later \ntrain['tweet'].apply(lambda x: len(str(x).split())).max()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:05.338587Z","iopub.execute_input":"2023-01-07T20:09:05.338908Z","iopub.status.idle":"2023-01-07T20:09:05.372213Z","shell.execute_reply.started":"2023-01-07T20:09:05.338877Z","shell.execute_reply":"2023-01-07T20:09:05.371148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Writing a function to getting AUC score for validation: \n# Understand ROC vs AUC score \ndef auc(pred, target):\n    fpr, tpr, threshold = metrics.roc_curve(target, pred)\n    return (metrics.auc(fpr, tpr))","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:10.041453Z","iopub.execute_input":"2023-01-07T20:09:10.041786Z","iopub.status.idle":"2023-01-07T20:09:10.046602Z","shell.execute_reply.started":"2023-01-07T20:09:10.041753Z","shell.execute_reply":"2023-01-07T20:09:10.045391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.tweet.values, train.hate.values, test_size = 0.2, random_state = 42, stratify= train.hate.values, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:28.219840Z","iopub.execute_input":"2023-01-07T20:09:28.220171Z","iopub.status.idle":"2023-01-07T20:09:28.231176Z","shell.execute_reply.started":"2023-01-07T20:09:28.220140Z","shell.execute_reply":"2023-01-07T20:09:28.230357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple RNN:\nRecurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer.\n\nWhy RNN's?\n\nhttps://www.quora.com/Why-do-we-use-an-RNN-instead-of-a-simple-neural-network","metadata":{}},{"cell_type":"code","source":"# Using Keras Tokenizer:\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntoken = Tokenizer()\nmax_len = 65\ntoken.fit_on_texts(list(xtrain) + list(xvalid))","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:37.830688Z","iopub.execute_input":"2023-01-07T20:09:37.831066Z","iopub.status.idle":"2023-01-07T20:09:38.062173Z","shell.execute_reply.started":"2023-01-07T20:09:37.831031Z","shell.execute_reply":"2023-01-07T20:09:38.061373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\nxtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:41.406991Z","iopub.execute_input":"2023-01-07T20:09:41.407340Z","iopub.status.idle":"2023-01-07T20:09:41.603868Z","shell.execute_reply.started":"2023-01-07T20:09:41.407305Z","shell.execute_reply":"2023-01-07T20:09:41.602678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = token.word_index","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:42.485882Z","iopub.execute_input":"2023-01-07T20:09:42.486213Z","iopub.status.idle":"2023-01-07T20:09:42.490501Z","shell.execute_reply.started":"2023-01-07T20:09:42.486182Z","shell.execute_reply":"2023-01-07T20:09:42.489377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RNN model \nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1, 300, input_length=65))\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:51.068783Z","iopub.execute_input":"2023-01-07T20:09:51.069121Z","iopub.status.idle":"2023-01-07T20:09:53.837489Z","shell.execute_reply.started":"2023-01-07T20:09:51.069089Z","shell.execute_reply":"2023-01-07T20:09:53.836756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, ytrain, epochs = 5)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:09:58.005390Z","iopub.execute_input":"2023-01-07T20:09:58.005743Z","iopub.status.idle":"2023-01-07T20:11:10.653142Z","shell.execute_reply.started":"2023-01-07T20:09:58.005711Z","shell.execute_reply":"2023-01-07T20:11:10.652365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(xvalid_pad)\nauc(scores, yvalid)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:11:10.655218Z","iopub.execute_input":"2023-01-07T20:11:10.655607Z","iopub.status.idle":"2023-01-07T20:11:10.997144Z","shell.execute_reply.started":"2023-01-07T20:11:10.655569Z","shell.execute_reply":"2023-01-07T20:11:10.996438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model1_rnn')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:14:55.434148Z","iopub.execute_input":"2023-01-07T20:14:55.434640Z","iopub.status.idle":"2023-01-07T20:14:57.361149Z","shell.execute_reply.started":"2023-01-07T20:14:55.434601Z","shell.execute_reply":"2023-01-07T20:14:57.360185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code Explanation:\nTokenization\n\nSo if you have watched the videos and referred to the links, you would know that in an RNN we input a sentence word by word. We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1.\nWhat keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.\nTry printing first 2 elements of xtrain_seq you will see every word is represented as a digit now.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_seq[:1]","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:11:10.998589Z","iopub.execute_input":"2023-01-07T20:11:10.998924Z","iopub.status.idle":"2023-01-07T20:11:11.004774Z","shell.execute_reply.started":"2023-01-07T20:11:10.998888Z","shell.execute_reply":"2023-01-07T20:11:11.003900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have done PADDING to make sequance equal inputs:\n#### Building the Neural Network\nTo understand the Dimensions of input and output given to RNN in keras her is a beautiful article : https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n\nThe first line model.Sequential() tells keras that we will be building our network sequentially . Then we first add the Embedding layer. Embedding layer is also a layer of neurons which takes in as input the nth dimensional one hot vector of every word and converts it into 300 dimensional vector , it gives us word embeddings similar to word2vec. We could have used word2vec but the embeddings layer learns during training to enhance the embeddings. Next we add an 100 RNN units without any dropout or regularization At last we add a single neuron with sigmoid function which takes output from 100 RNN cells (Please note we have 100 LSTM cells not layers) to predict the results and then we compile the model using adam optimizer..\n\n### Comments on the model\n\nWe can see our model achieves an accuracy of 1 which is just insane , we are clearly overfitting I know , but this was the simplest model of all ,we can tune a lot of hyperparameters like RNN units, we can do batch normalization , dropouts etc to get better result. The point is we got an AUC score of 0.82 without much efforts and we know have learnt about RNN's .Deep learning is really revolutionary","metadata":{}},{"cell_type":"markdown","source":"## Word Embeddings: \n\nWhile building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings? Here is the answer :\n\nhttps://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation\nhttps://machinelearningmastery.com/what-are-word-embeddings/\n\nThe latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip or you can search for GloVe in datasets on Kaggle and add the file","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/glove.6B.zip.1\n!rm -rf /kaggle/working/glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:20:11.493385Z","iopub.execute_input":"2023-01-07T20:20:11.493761Z","iopub.status.idle":"2023-01-07T20:20:13.639898Z","shell.execute_reply.started":"2023-01-07T20:20:11.493728Z","shell.execute_reply":"2023-01-07T20:20:13.638780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:20:18.202372Z","iopub.execute_input":"2023-01-07T20:20:18.202731Z","iopub.status.idle":"2023-01-07T20:22:58.549556Z","shell.execute_reply.started":"2023-01-07T20:20:18.202699Z","shell.execute_reply":"2023-01-07T20:22:58.548754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /kaggle/working/glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:23:38.959083Z","iopub.execute_input":"2023-01-07T20:23:38.959467Z","iopub.status.idle":"2023-01-07T20:24:00.943401Z","shell.execute_reply.started":"2023-01-07T20:23:38.959420Z","shell.execute_reply":"2023-01-07T20:24:00.942501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Glove model in Dictionary: \nglove = {}\nf = open('/kaggle/working/glove.6B.300d.txt', 'r')\nfor line in f:\n    line = line.split(' ')\n    word = line[0]\n    coeff = np.asarray([float(val) for val in line[1:]])\n    glove[word] = coeff\nf.close()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:25:31.686974Z","iopub.execute_input":"2023-01-07T20:25:31.687327Z","iopub.status.idle":"2023-01-07T20:26:08.060975Z","shell.execute_reply.started":"2023-01-07T20:25:31.687291Z","shell.execute_reply":"2023-01-07T20:26:08.060049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Embedding metrics for words we have in dataset: \nembedding_metrics = np.zeros((len(word_index)+1, 300))\nfor word, i in word_index.items():\n    embedding = glove.get(word)\n    if embedding is not None:\n        embedding_metrics[i] = embedding","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:29:17.511169Z","iopub.execute_input":"2023-01-07T20:29:17.511575Z","iopub.status.idle":"2023-01-07T20:29:17.530096Z","shell.execute_reply.started":"2023-01-07T20:29:17.511544Z","shell.execute_reply":"2023-01-07T20:29:17.529325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM's:\n\nSimple RNN's were certainly better than classical ML algorithms and gave state of the art results, but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM's were introduced to counter to these drawbacks.\n\n#### Code Implementation\nWe have already tokenized and paded our text for input to LSTM's\n\n","metadata":{}},{"cell_type":"code","source":"# LSTM model \nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1, 300, weights=[embedding_metrics], input_length=65, trainable=False))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:29:21.686717Z","iopub.execute_input":"2023-01-07T20:29:21.687044Z","iopub.status.idle":"2023-01-07T20:29:21.908533Z","shell.execute_reply.started":"2023-01-07T20:29:21.687013Z","shell.execute_reply":"2023-01-07T20:29:21.907836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, ytrain, epochs = 5)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:29:29.612379Z","iopub.execute_input":"2023-01-07T20:29:29.612724Z","iopub.status.idle":"2023-01-07T20:32:13.517170Z","shell.execute_reply.started":"2023-01-07T20:29:29.612693Z","shell.execute_reply":"2023-01-07T20:32:13.516355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(xvalid_pad)\nauc(scores, yvalid)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:32:13.519040Z","iopub.execute_input":"2023-01-07T20:32:13.519391Z","iopub.status.idle":"2023-01-07T20:32:14.212789Z","shell.execute_reply.started":"2023-01-07T20:32:13.519352Z","shell.execute_reply":"2023-01-07T20:32:14.211961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model2_lstm')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:32:14.215148Z","iopub.execute_input":"2023-01-07T20:32:14.215694Z","iopub.status.idle":"2023-01-07T20:32:17.101804Z","shell.execute_reply.started":"2023-01-07T20:32:14.215652Z","shell.execute_reply":"2023-01-07T20:32:17.100992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code Explanation\nAs a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors . Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable = False. Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units\n\nComments on the Model\nWe now see that the model is not overfitting and achieves an auc score of 0.96 which is quite commendable , also we close in on the gap between accuracy and auc . We see that in this case we used dropout and prevented overfitting the data","metadata":{}},{"cell_type":"markdown","source":"## GRU's\n### Basic Overview\nIntroduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_metrics],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:32:31.522359Z","iopub.execute_input":"2023-01-07T20:32:31.522686Z","iopub.status.idle":"2023-01-07T20:32:31.806828Z","shell.execute_reply.started":"2023-01-07T20:32:31.522656Z","shell.execute_reply":"2023-01-07T20:32:31.806093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, ytrain, epochs = 5)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:32:34.947680Z","iopub.execute_input":"2023-01-07T20:32:34.948002Z","iopub.status.idle":"2023-01-07T20:32:43.703095Z","shell.execute_reply.started":"2023-01-07T20:32:34.947972Z","shell.execute_reply":"2023-01-07T20:32:43.702283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (auc(scores,yvalid)))","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:32:43.707132Z","iopub.execute_input":"2023-01-07T20:32:43.709190Z","iopub.status.idle":"2023-01-07T20:32:44.099591Z","shell.execute_reply.started":"2023-01-07T20:32:43.709147Z","shell.execute_reply":"2023-01-07T20:32:44.098082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model3_gru')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-07T20:33:34.779169Z","iopub.execute_input":"2023-01-07T20:33:34.779712Z","iopub.status.idle":"2023-01-07T20:33:38.395683Z","shell.execute_reply.started":"2023-01-07T20:33:34.779674Z","shell.execute_reply":"2023-01-07T20:33:38.394343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bi-Directional RNN's:\nCode is same as before,only we have added bidirectional nature to the LSTM cells we used before and is self explanatory. We have achieve similar accuracy and auc score as before and now we have learned all the types of typical RNN architectures","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_metrics],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \n    \nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IT takes more time so i did not implement this","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}