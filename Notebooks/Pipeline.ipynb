{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piplene of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\makch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, AutoModel, AutoTokenizer, BertTokenizerFast\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                            ّ    | # Tashdid\n",
    "                            َ    | # Fatha\n",
    "                            ً    | # Tanwin Fath\n",
    "                            ُ    | # Damma\n",
    "                            ٌ    | # Tanwin Damm\n",
    "                            ِ    | # Kasra\n",
    "                            ٍ    | # Tanwin Kasr\n",
    "                            ْ    | # Sukun\n",
    "                            ـ     # Tatwil/Kashida\n",
    "                        \"\"\", re.VERBOSE)\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations = arabic_punctuations + english_punctuations\n",
    "\n",
    "\n",
    "def remove_urls (text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emails(text):\n",
    "    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "# def remove_emoji(text):\n",
    "#     return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "def remove_emoji(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def normalization(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_sentence = [w for w in text.split() if not w in arabic_stopwords]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def cleaning_content(line):\n",
    "    if (isinstance(line, float)):\n",
    "        return None\n",
    "    line.replace('\\n', ' ')\n",
    "    line = remove_emails(line)\n",
    "    line = remove_urls(line)\n",
    "    line = remove_emoji(line)\n",
    "    nline = [w if '@' not in w else 'USERID' for w in line.split()]\n",
    "    line = ' '.join(nline)\n",
    "    line = line.replace('RT', '').replace('<LF>', '').replace('<br />','').replace('&quot;', '').replace('<url>', '').replace('USERID', '')\n",
    "\n",
    "\n",
    "    # add spaces between punc,\n",
    "    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations}))\n",
    "\n",
    "    # then remove punc,\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    line = line.translate(translator)\n",
    "\n",
    "    line = remove_stopwords(line)\n",
    "    line=remove_diacritics(normalization(line))\n",
    "\n",
    "    line = line.strip()\n",
    "    return line\n",
    "\n",
    "def hasDigits(s):\n",
    "    return any( 48 <= ord(char) <= 57  or 1632 <= ord(char) <= 1641 for char in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offensive detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        #  hidden size of BERT, hidden size of our classifier, number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                        max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                        information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                        num_labels)\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)  \n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task and feed them to classifier to compute logits \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dictionary\n",
    "\n",
    "with open('../models/offensive_dict.pkl', 'rb') as handle:\n",
    "    off_dictionary = pickle.load(handle)\n",
    "\n",
    "off_model = BertClassifier()\n",
    "off_model.load_state_dict(torch.load('../models/modelv3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN =  256\n",
    "\n",
    "# Load MAX_LEN\n",
    "with open('../models/offensive_max_len.pkl', 'rb') as handle:\n",
    "    MAX_LEN = pickle.load(handle)\n",
    "\n",
    "\n",
    "def predict_off(review_text,model,device,tokenizer):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    # decode the output of the model to get the predicted label\n",
    "    pred = off_dictionary[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Racism detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "racism_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-medium-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Racism_Model(pl.LightningModule):\n",
    "    def __init__(self,model_type=\"Mini\"):\n",
    "        super().__init__()\n",
    "        model = {\"Mini\": (\"asafaya/bert-mini-arabic\",256),\n",
    "                \"Medium\": (\"asafaya/bert-medium-arabic\",512),\n",
    "                \"Base\": (\"asafaya/bert-base-arabic\",768),\n",
    "                \"Large\": (\"asafaya/bert-large-arabic\",1024)}\n",
    "        self.bert_model = AutoModel.from_pretrained(model[model_type][0])\n",
    "        self.fc = nn.Linear(model[model_type][1],18)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        out = self.bert_model(input_ids = input_ids, attention_mask =attention_mask)#inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])\n",
    "        pooler = out[1]\n",
    "        out = self.fc(pooler)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at asafaya/bert-medium-arabic were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "racism_model = Racism_Model('Medium')\n",
    "racism_model.load_state_dict(torch.load('../models/racism/racism_arabert.pt'))\n",
    "\n",
    "# dictionary\n",
    "racism_dict = {0: 'Not_Racism', 1: 'Racism'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load max len\n",
    "with open('../models/racism/racism_arabert_maxlen.pickle', 'rb') as handle:\n",
    "    racism_max_len = pickle.load(handle)\n",
    "\n",
    "def predict_racism(review_text,model,device,tokenizer):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=racism_max_len,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    # decode the output of the model to get the predicted label\n",
    "    pred = racism_dict[index]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Religion Hate detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Religion_Hate_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-medium-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReligionHateModel(pl.LightningModule):\n",
    "    def __init__(self,model_type=\"Mini\"):\n",
    "        super().__init__()\n",
    "        model = {\"Mini\": (\"asafaya/bert-mini-arabic\",256),\n",
    "                \"Medium\": (\"asafaya/bert-medium-arabic\",512),\n",
    "                \"Base\": (\"asafaya/bert-base-arabic\",768),\n",
    "                \"Large\": (\"asafaya/bert-large-arabic\",1024)}\n",
    "        self.bert_model = AutoModel.from_pretrained(model[model_type][0])\n",
    "        self.fc = nn.Linear(model[model_type][1],18)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        out = self.bert_model(input_ids = input_ids, attention_mask =attention_mask)#inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])\n",
    "        pooler = out[1]\n",
    "        out = self.fc(pooler)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at asafaya/bert-medium-arabic were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "Religion_Hate_model = ReligionHateModel('Medium')\n",
    "Religion_Hate_model.load_state_dict(torch.load('../models/religion_hate/religion_hate_params.pt'))\n",
    "\n",
    "# dictionary\n",
    "Religion_Hate_dict = {0: 'Religion Hate', 1: 'Not Religion Hate'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Religion_Hate_predict(review_text,model,device,tokenizer):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=60,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    # decode the output of the model to get the predicted label\n",
    "    pred = Religion_Hate_dict[index]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbal Abuse detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbal_abuse_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-medium-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class verbalabuseModel(pl.LightningModule):\n",
    "    def __init__(self,model_type=\"Mini\"):\n",
    "        super().__init__()\n",
    "        model = {\"Mini\": (\"asafaya/bert-mini-arabic\",256),\n",
    "                \"Medium\": (\"asafaya/bert-medium-arabic\",512),\n",
    "                \"Base\": (\"asafaya/bert-base-arabic\",768),\n",
    "                \"Large\": (\"asafaya/bert-large-arabic\",1024)}\n",
    "        self.bert_model = AutoModel.from_pretrained(model[model_type][0])\n",
    "        self.fc = nn.Linear(model[model_type][1],18)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        out = self.bert_model(input_ids = input_ids, attention_mask =attention_mask)#inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])\n",
    "        pooler = out[1]\n",
    "        out = self.fc(pooler)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at asafaya/bert-medium-arabic were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "verbal_abuse_model = verbalabuseModel('Medium')\n",
    "verbal_abuse_model.load_state_dict(torch.load('../models/verbal_abuse/verbal_abuse_arabert.pt'))\n",
    "\n",
    "# dictionary\n",
    "verbal_abuse_dict = {0: 'Verbal Abuse', 1: 'Not Verbal Abuse'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_verbal_abuse(review_text,model,device,tokenizer):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=60,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    # decode the output of the model to get the predicted label\n",
    "    pred = verbal_abuse_dict[index]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misogony detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "misogyny_tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-medium-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicBertModel(pl.LightningModule):\n",
    "    def __init__(self,model_type=\"Mini\"):\n",
    "        super().__init__()\n",
    "        model = {\"Mini\": (\"asafaya/bert-mini-arabic\",256),\n",
    "                \"Medium\": (\"asafaya/bert-medium-arabic\",512),\n",
    "                \"Base\": (\"asafaya/bert-base-arabic\",768),\n",
    "                \"Large\": (\"asafaya/bert-large-arabic\",1024)}\n",
    "        self.bert_model = AutoModel.from_pretrained(model[model_type][0])\n",
    "        self.fc = nn.Linear(model[model_type][1],18)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        out = self.bert_model(input_ids = input_ids, attention_mask =attention_mask)#inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])\n",
    "        pooler = out[1]\n",
    "        out = self.fc(pooler)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "misogyny_dict = {0: 'misogyny', 1: 'non_misogyny'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at asafaya/bert-medium-arabic were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misogyny_model = ArabicBertModel('Medium')\n",
    "misogyny_model.load_state_dict(torch.load('../models/misogyny/misogyny.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_misogyny(review_text,model,device,tokenizer):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=60,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    # decode the output of the model to get the predicted label\n",
    "    pred = misogyny_dict[index]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialect detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'alger-ia/dziribert'\n",
    "class TweetModule(pl.LightningModule):\n",
    "  def __init__(self, n_classes):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.bert(input_ids, attention_mask)\n",
    "    output = self.classifier(output.pooler_output)                    \n",
    "    # if provided with labels return loss and output\n",
    "    if labels is not None:\n",
    "      loss = self.criterion(output, labels)\n",
    "      return loss, output \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at alger-ia/dziribert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dialect = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# load the model\n",
    "dialect_dict = {0: 'lebanon', 1: 'egypt', 2: 'morocco', 3: 'tunisia', 4: 'algeria', 5: 'qatar', 6: 'iraq', 7: 'saudi arabia', 8: 'libya', 9: 'jordan'}\n",
    "\n",
    "dialect_model = TweetModule(10)\n",
    "dialect_model.load_state_dict(torch.load('../models/dialect_classifier.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dialect(review_text,model,device,tokenizer):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=123,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    pred = dialect_dict[index]\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "\n",
    "def predict(text):\n",
    "    # clean text\n",
    "    text = cleaning_content(text)\n",
    "    \n",
    "    # predict using offensive model\n",
    "    off_pred = predict_off(text,off_model,device,tokenizer)\n",
    "\n",
    "    if off_pred == 'offensive':\n",
    "        # predict using racism model\n",
    "        rac_pred = predict_racism(text,racism_model,device,racism_tokenizer)\n",
    "        # predict using misogyny model\n",
    "        misog_pred = predict_misogyny(text,misogyny_model,device,misogyny_tokenizer)\n",
    "        # predict using verbal abuse model\n",
    "        ver_pred = predict_verbal_abuse(text,verbal_abuse_model,device,verbal_abuse_tokenizer)\n",
    "        # predict using dialect model\n",
    "        dialect_pred = predict_dialect(text,dialect_model,device,tokenizer_dialect)\n",
    "        # predict using religion hate model\n",
    "        Religion_Hate_pred = Religion_Hate_predict(text,Religion_Hate_model,device,Religion_Hate_tokenizer)\n",
    "        # return the prediction\n",
    "        return {\"Offensiveness\": off_pred, \"Dialect\": dialect_pred, \"Misogyny\": misog_pred, \"Racism\": rac_pred, \"Verbal Abuse\": ver_pred, \"Religion Hate\": Religion_Hate_pred}\n",
    "    \n",
    "    # predict using misogyny model\n",
    "    misog_pred = predict_misogyny(text,misogyny_model,device,misogyny_tokenizer)\n",
    "\n",
    "    # predict using dialect model\n",
    "    dialect_pred = predict_dialect(text,dialect_model,device,tokenizer_dialect)\n",
    "    \n",
    "    # return the prediction  as a dataframe row\n",
    "    return {\"Offensiveness\": off_pred, \"Dialect\": dialect_pred, \"Misogyny\": misog_pred, \"Racism\": \"Not_Racism\", \"Verbal Abuse\": \"Not Verbal Abuse\", \"Religion Hate\": \"Not Religion Hate\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'non_offensive',\n",
       " 'Dialect': 'tunisia',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "text = 'كل العرب يحبون السعودية'\n",
    "\n",
    "predict(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'algeria',\n",
       " 'Misogyny': 'misogyny',\n",
       " 'Racism': 'Racism',\n",
       " 'Verbal Abuse': 'Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'اذهب لبلدك يا اسود أنت و شعبك المعفن'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'non_offensive',\n",
       " 'Dialect': 'tunisia',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'هنالك قط اسود في المنزل'\n",
    "\n",
    "# predict\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'tunisia',\n",
       " 'Misogyny': 'misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'موتي يا حمارة'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'egypt',\n",
       " 'Misogyny': 'misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'نيك امك يا ساقطة'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'non_offensive',\n",
       " 'Dialect': 'algeria',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'انتي مستواك لا يسمح لك ان تكوني في هادا المكان'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'tunisia',\n",
       " 'Misogyny': 'misogyny',\n",
       " 'Racism': 'Racism',\n",
       " 'Verbal Abuse': 'Verbal Abuse',\n",
       " 'Religion Hate': 'Religion Hate'}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'اذا عربت خربت اينما حل الاعراب حل الحراب'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'iraq',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Racism',\n",
       " 'Verbal Abuse': 'Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'السعوديه هي اطهر ارض وشعب واحقر شعب'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'non_offensive',\n",
       " 'Dialect': 'iraq',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'سوريا اجمل بلد في العالم'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'morocco',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Verbal Abuse',\n",
       " 'Religion Hate': 'Religion Hate'}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ان الله غفور رحيم كلنا عندنا عيوبنا داك الشماته اللي صور الفيديو سير الله يفضحك دنيا واخيره'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'saudi arabia',\n",
       " 'Misogyny': 'misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Religion Hate'}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ام نص لسان ابكي بترتاحي'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'iraq',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Not_Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'خسات نجس المملكه العربيه السعوديه الاسلاميه قويه بتوحيدها وقادهعلي ردع تسول نفسه اللعب امنها والسعوديه ايران الرخوه لتطلب مسانده احد'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Offensiveness': 'offensive',\n",
       " 'Dialect': 'egypt',\n",
       " 'Misogyny': 'non_misogyny',\n",
       " 'Racism': 'Racism',\n",
       " 'Verbal Abuse': 'Not Verbal Abuse',\n",
       " 'Religion Hate': 'Not Religion Hate'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'اساسا انتم تدرون ان سلاطين اراضيكم الجاهليه ماضغطونا اظن والله اعلم ان كلمه ساضغط مشتقه ضغط الجزيره مرتفع وظهرت بسببه الكلمه'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_data = pd.read_csv('../DataSet/test-data.csv')\n",
    "\n",
    "# clean the data\n",
    "\n",
    "test_data['Text'] = test_data['Text'].apply(cleaning_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ناقصه عقل ودين صاحبه المنشور</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...    NaN\n",
       "1  تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...    NaN\n",
       "2                       ناقصه عقل ودين صاحبه المنشور    NaN\n",
       "3  اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...    NaN\n",
       "4  الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...    NaN"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Offensiveness</th>\n",
       "      <th>Dialect</th>\n",
       "      <th>Misogyny</th>\n",
       "      <th>Racism</th>\n",
       "      <th>Verbal Abuse</th>\n",
       "      <th>Religion Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non_offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non_offensive</td>\n",
       "      <td>saudi arabia</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non_offensive</td>\n",
       "      <td>iraq</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Offensiveness       Dialect      Misogyny      Racism      Verbal Abuse  \\\n",
       "0  non_offensive       tunisia  non_misogyny  Not_Racism  Not Verbal Abuse   \n",
       "1      offensive       tunisia      misogyny  Not_Racism  Not Verbal Abuse   \n",
       "2      offensive       tunisia      misogyny  Not_Racism      Verbal Abuse   \n",
       "3  non_offensive  saudi arabia  non_misogyny  Not_Racism  Not Verbal Abuse   \n",
       "4  non_offensive          iraq  non_misogyny  Not_Racism  Not Verbal Abuse   \n",
       "\n",
       "       Religion Hate  \n",
       "0  Not Religion Hate  \n",
       "1  Not Religion Hate  \n",
       "2  Not Religion Hate  \n",
       "3  Not Religion Hate  \n",
       "4  Not Religion Hate  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the data\n",
    "\n",
    "\n",
    "#empty dataframe\n",
    "df = pd.DataFrame(columns=['Offensiveness', 'Dialect', 'Misogyny', 'Racism', 'Verbal Abuse', 'Religion Hate'])\n",
    "\n",
    "# loop over the data\n",
    "for i in range(len(test_data)):\n",
    "    # get the text\n",
    "    text = test_data['Text'][i]\n",
    "    # predict and create a dataframe row\n",
    "    row = predict(text)\n",
    "    # append the row to the dataframe using concat\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Offensiveness</th>\n",
       "      <th>Dialect</th>\n",
       "      <th>Misogyny</th>\n",
       "      <th>Racism</th>\n",
       "      <th>Verbal Abuse</th>\n",
       "      <th>Religion Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non_offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ناقصه عقل ودين صاحبه المنشور</td>\n",
       "      <td>NaN</td>\n",
       "      <td>offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non_offensive</td>\n",
       "      <td>saudi arabia</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non_offensive</td>\n",
       "      <td>iraq</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label  Offensiveness  \\\n",
       "0  بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...    NaN  non_offensive   \n",
       "1  تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...    NaN      offensive   \n",
       "2                       ناقصه عقل ودين صاحبه المنشور    NaN      offensive   \n",
       "3  اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...    NaN  non_offensive   \n",
       "4  الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...    NaN  non_offensive   \n",
       "\n",
       "        Dialect      Misogyny      Racism      Verbal Abuse      Religion Hate  \n",
       "0       tunisia  non_misogyny  Not_Racism  Not Verbal Abuse  Not Religion Hate  \n",
       "1       tunisia      misogyny  Not_Racism  Not Verbal Abuse  Not Religion Hate  \n",
       "2       tunisia      misogyny  Not_Racism      Verbal Abuse  Not Religion Hate  \n",
       "3  saudi arabia  non_misogyny  Not_Racism  Not Verbal Abuse  Not Religion Hate  \n",
       "4          iraq  non_misogyny  Not_Racism  Not Verbal Abuse  Not Religion Hate  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.concat([test_data, df], axis=1)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"NLP\")\n",
    "\n",
    "def geolocate(country):\n",
    "    try:\n",
    "        # Geolocate the center of the country\n",
    "        loc = geolocator.geocode(country)\n",
    "        # And return latitude and longitude\n",
    "        return (loc.latitude, loc.longitude)\n",
    "    except:\n",
    "        # Return missing value\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the location of each country\n",
    "test_data['Location'] = test_data['Dialect'].apply(lambda x: geolocate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Offensiveness</th>\n",
       "      <th>Dialect</th>\n",
       "      <th>Misogyny</th>\n",
       "      <th>Racism</th>\n",
       "      <th>Verbal Abuse</th>\n",
       "      <th>Religion Hate</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non_offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "      <td>(33.8439408, 9.400138)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "      <td>(33.8439408, 9.400138)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ناقصه عقل ودين صاحبه المنشور</td>\n",
       "      <td>NaN</td>\n",
       "      <td>offensive</td>\n",
       "      <td>tunisia</td>\n",
       "      <td>misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "      <td>(33.8439408, 9.400138)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non_offensive</td>\n",
       "      <td>saudi arabia</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "      <td>(25.6242618, 42.3528328)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non_offensive</td>\n",
       "      <td>iraq</td>\n",
       "      <td>non_misogyny</td>\n",
       "      <td>Not_Racism</td>\n",
       "      <td>Not Verbal Abuse</td>\n",
       "      <td>Not Religion Hate</td>\n",
       "      <td>(33.0955793, 44.1749775)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label  Offensiveness  \\\n",
       "0  بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...    NaN  non_offensive   \n",
       "1  تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...    NaN      offensive   \n",
       "2                       ناقصه عقل ودين صاحبه المنشور    NaN      offensive   \n",
       "3  اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...    NaN  non_offensive   \n",
       "4  الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...    NaN  non_offensive   \n",
       "\n",
       "        Dialect      Misogyny      Racism      Verbal Abuse  \\\n",
       "0       tunisia  non_misogyny  Not_Racism  Not Verbal Abuse   \n",
       "1       tunisia      misogyny  Not_Racism  Not Verbal Abuse   \n",
       "2       tunisia      misogyny  Not_Racism      Verbal Abuse   \n",
       "3  saudi arabia  non_misogyny  Not_Racism  Not Verbal Abuse   \n",
       "4          iraq  non_misogyny  Not_Racism  Not Verbal Abuse   \n",
       "\n",
       "       Religion Hate                  Location  \n",
       "0  Not Religion Hate    (33.8439408, 9.400138)  \n",
       "1  Not Religion Hate    (33.8439408, 9.400138)  \n",
       "2  Not Religion Hate    (33.8439408, 9.400138)  \n",
       "3  Not Religion Hate  (25.6242618, 42.3528328)  \n",
       "4  Not Religion Hate  (33.0955793, 44.1749775)  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total text</th>\n",
       "      <th>Offensiveness</th>\n",
       "      <th>Racism</th>\n",
       "      <th>Verbal Abuse</th>\n",
       "      <th>Religion Hate</th>\n",
       "      <th>Misogyny</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dialect</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>algeria</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(28.0000272, 2.9999825)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>egypt</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(26.2540493, 29.2675469)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iraq</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(33.0955793, 44.1749775)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libya</th>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>(26.8234472, 18.1236723)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morocco</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(31.1728205, -7.3362482)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saudi arabia</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(25.6242618, 42.3528328)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunisia</th>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(33.8439408, 9.400138)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Total text  Offensiveness  Racism  Verbal Abuse  Religion Hate  \\\n",
       "Dialect                                                                        \n",
       "algeria                2            0.0     0.0           0.0              0   \n",
       "egypt                  4            3.0     0.0           2.0              0   \n",
       "iraq                   2            0.0     0.0           0.0              0   \n",
       "libya                 16            5.0     2.0           3.0              0   \n",
       "morocco                1            1.0     0.0           0.0              0   \n",
       "saudi arabia           3            0.0     0.0           0.0              0   \n",
       "tunisia                9            3.0     0.0           1.0              0   \n",
       "\n",
       "              Misogyny                  Location  \n",
       "Dialect                                           \n",
       "algeria            1.0   (28.0000272, 2.9999825)  \n",
       "egypt              2.0  (26.2540493, 29.2675469)  \n",
       "iraq               0.0  (33.0955793, 44.1749775)  \n",
       "libya              6.0  (26.8234472, 18.1236723)  \n",
       "morocco            0.0  (31.1728205, -7.3362482)  \n",
       "saudi arabia       0.0  (25.6242618, 42.3528328)  \n",
       "tunisia            5.0    (33.8439408, 9.400138)  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by dialect and count the number of tweets with offensiveness = offensive\n",
    "dialect_off = test_data.groupby('Dialect')['Offensiveness'].value_counts().unstack().fillna(0)['offensive'] if 'offensive' in test_data.groupby('Dialect')['Offensiveness'].value_counts().unstack().columns else 0\n",
    "\n",
    "misogyny_off = test_data.groupby('Dialect')['Misogyny'].value_counts().unstack().fillna(0)['misogyny'] if 'misogyny' in test_data.groupby('Dialect')['Misogyny'].value_counts().unstack().columns else 0\n",
    "\n",
    "racism_off = test_data.groupby('Dialect')['Racism'].value_counts().unstack().fillna(0)['Racism'] if 'Racism' in test_data.groupby('Dialect')['Racism'].value_counts().unstack().columns else 0\n",
    "\n",
    "verbal_off = test_data.groupby('Dialect')['Verbal Abuse'].value_counts().unstack().fillna(0)['Verbal Abuse'] if 'Verbal Abuse' in test_data.groupby('Dialect')['Verbal Abuse'].value_counts().unstack().columns else 0\n",
    "\n",
    "religion_off = test_data.groupby('Dialect')['Religion Hate'].value_counts().unstack().fillna(0)['Religion Hate'] if 'Religion Hate' in test_data.groupby('Dialect')['Religion Hate'].value_counts().unstack().columns else 0\n",
    "\n",
    "# sym of tweets for each dialect\n",
    "dialect_sym = test_data.groupby('Dialect')['Text'].count()\n",
    "\n",
    "# create a dataframe\n",
    "dialect_df = pd.DataFrame({'Total text' : dialect_sym,'Offensiveness':dialect_off, 'Racism':racism_off, 'Verbal Abuse':verbal_off, 'Religion Hate':religion_off, 'Misogyny':misogyny_off, 'Location':test_data.groupby('Dialect')['Location'].first()})\n",
    "\n",
    "dialect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect_df['Total text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_8808f79cb3c120870045aef59d332bf0 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_8808f79cb3c120870045aef59d332bf0&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_8808f79cb3c120870045aef59d332bf0 = L.map(\n",
       "                &quot;map_8808f79cb3c120870045aef59d332bf0&quot;,\n",
       "                {\n",
       "                    center: [27.0, 48.0],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 4,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_86a2c57f4b0028cd0598c6f51f03eeee = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "            var marker_a88ba282f1c92779f078b9638e7d71fe = L.marker(\n",
       "                [28.0000272, 2.9999825],\n",
       "                {}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "        var popup_334d5e182c5d07a76659f27d5d786a50 = L.popup({&quot;maxWidth&quot;: 300});\n",
       "\n",
       "        \n",
       "            \n",
       "                var i_frame_2462aa7a1301799bacf1cc02ecac1e63 = $(`&lt;iframe src=&quot;data:text/html;charset=utf-8;base64,CiAgICA8Yj4gTnVtYmVyIG9mIHR3ZWV0czogPC9iPiAyLCAgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIG9mZmVuc2l2ZSB0d2VldHM6PC9iPiAwLjAsIDxicj4gPGI+VGhlIG51bWJlciBvZiByYWNpc20gdHdlZXRzOjwvYj4gMC4wLCA8YnI+IDxiPlRoZSBudW1iZXIgb2YgdmVyYmFsIGFidXNlIHR3ZWV0czo8L2I+IDAuMCwgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIHJlbGlnaW9uIGhhdGUgdHdlZXRzOjwvYj4gMCwgPGJyPiA8Yj4gVGhlIG51bWJlciBvZiBtaXNvZ3lueSB0d2VldHM6PC9iPiAxLjA=&quot; width=&quot;300&quot; style=&quot;border:none !important;&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;`)[0];\n",
       "                popup_334d5e182c5d07a76659f27d5d786a50.setContent(i_frame_2462aa7a1301799bacf1cc02ecac1e63);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_a88ba282f1c92779f078b9638e7d71fe.bindPopup(popup_334d5e182c5d07a76659f27d5d786a50)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_4122a75601d07d14772ce5762cad7108 = L.marker(\n",
       "                [26.2540493, 29.2675469],\n",
       "                {}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "        var popup_2c5fcce051abfe95e46d9057b1095d6c = L.popup({&quot;maxWidth&quot;: 300});\n",
       "\n",
       "        \n",
       "            \n",
       "                var i_frame_418a41d13a1965bd22b0133874292367 = $(`&lt;iframe src=&quot;data:text/html;charset=utf-8;base64,CiAgICA8Yj4gTnVtYmVyIG9mIHR3ZWV0czogPC9iPiA0LCAgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIG9mZmVuc2l2ZSB0d2VldHM6PC9iPiAzLjAsIDxicj4gPGI+VGhlIG51bWJlciBvZiByYWNpc20gdHdlZXRzOjwvYj4gMC4wLCA8YnI+IDxiPlRoZSBudW1iZXIgb2YgdmVyYmFsIGFidXNlIHR3ZWV0czo8L2I+IDIuMCwgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIHJlbGlnaW9uIGhhdGUgdHdlZXRzOjwvYj4gMCwgPGJyPiA8Yj4gVGhlIG51bWJlciBvZiBtaXNvZ3lueSB0d2VldHM6PC9iPiAyLjA=&quot; width=&quot;300&quot; style=&quot;border:none !important;&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;`)[0];\n",
       "                popup_2c5fcce051abfe95e46d9057b1095d6c.setContent(i_frame_418a41d13a1965bd22b0133874292367);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_4122a75601d07d14772ce5762cad7108.bindPopup(popup_2c5fcce051abfe95e46d9057b1095d6c)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_1a5888094b299bcdbb23a399bf8495da = L.marker(\n",
       "                [33.0955793, 44.1749775],\n",
       "                {}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "        var popup_7808d1eea695cecf25b8de1eb3267d50 = L.popup({&quot;maxWidth&quot;: 300});\n",
       "\n",
       "        \n",
       "            \n",
       "                var i_frame_edccebc0a05603af46c45a7ea7c5f8ce = $(`&lt;iframe src=&quot;data:text/html;charset=utf-8;base64,CiAgICA8Yj4gTnVtYmVyIG9mIHR3ZWV0czogPC9iPiAyLCAgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIG9mZmVuc2l2ZSB0d2VldHM6PC9iPiAwLjAsIDxicj4gPGI+VGhlIG51bWJlciBvZiByYWNpc20gdHdlZXRzOjwvYj4gMC4wLCA8YnI+IDxiPlRoZSBudW1iZXIgb2YgdmVyYmFsIGFidXNlIHR3ZWV0czo8L2I+IDAuMCwgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIHJlbGlnaW9uIGhhdGUgdHdlZXRzOjwvYj4gMCwgPGJyPiA8Yj4gVGhlIG51bWJlciBvZiBtaXNvZ3lueSB0d2VldHM6PC9iPiAwLjA=&quot; width=&quot;300&quot; style=&quot;border:none !important;&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;`)[0];\n",
       "                popup_7808d1eea695cecf25b8de1eb3267d50.setContent(i_frame_edccebc0a05603af46c45a7ea7c5f8ce);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_1a5888094b299bcdbb23a399bf8495da.bindPopup(popup_7808d1eea695cecf25b8de1eb3267d50)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_6f6826141cc34d75ad0144df4d643b33 = L.marker(\n",
       "                [26.8234472, 18.1236723],\n",
       "                {}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "        var popup_81f38cb70d8ddb2a1dc815048299ee80 = L.popup({&quot;maxWidth&quot;: 300});\n",
       "\n",
       "        \n",
       "            \n",
       "                var i_frame_c061a5af0d59b447f6bf30e7ac28ec7e = $(`&lt;iframe src=&quot;data:text/html;charset=utf-8;base64,CiAgICA8Yj4gTnVtYmVyIG9mIHR3ZWV0czogPC9iPiAxNiwgIDxicj4gPGI+VGhlIG51bWJlciBvZiBvZmZlbnNpdmUgdHdlZXRzOjwvYj4gNS4wLCA8YnI+IDxiPlRoZSBudW1iZXIgb2YgcmFjaXNtIHR3ZWV0czo8L2I+IDIuMCwgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIHZlcmJhbCBhYnVzZSB0d2VldHM6PC9iPiAzLjAsIDxicj4gPGI+VGhlIG51bWJlciBvZiByZWxpZ2lvbiBoYXRlIHR3ZWV0czo8L2I+IDAsIDxicj4gPGI+IFRoZSBudW1iZXIgb2YgbWlzb2d5bnkgdHdlZXRzOjwvYj4gNi4w&quot; width=&quot;300&quot; style=&quot;border:none !important;&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;`)[0];\n",
       "                popup_81f38cb70d8ddb2a1dc815048299ee80.setContent(i_frame_c061a5af0d59b447f6bf30e7ac28ec7e);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_6f6826141cc34d75ad0144df4d643b33.bindPopup(popup_81f38cb70d8ddb2a1dc815048299ee80)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_c5871e50b7b1447114b71d4015ca165a = L.marker(\n",
       "                [31.1728205, -7.3362482],\n",
       "                {}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "        var popup_00bcd8e53fc3c1a077b081ef0b3d73a1 = L.popup({&quot;maxWidth&quot;: 300});\n",
       "\n",
       "        \n",
       "            \n",
       "                var i_frame_ef6aba99c21b474a383c6040010ffc99 = $(`&lt;iframe src=&quot;data:text/html;charset=utf-8;base64,CiAgICA8Yj4gTnVtYmVyIG9mIHR3ZWV0czogPC9iPiAxLCAgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIG9mZmVuc2l2ZSB0d2VldHM6PC9iPiAxLjAsIDxicj4gPGI+VGhlIG51bWJlciBvZiByYWNpc20gdHdlZXRzOjwvYj4gMC4wLCA8YnI+IDxiPlRoZSBudW1iZXIgb2YgdmVyYmFsIGFidXNlIHR3ZWV0czo8L2I+IDAuMCwgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIHJlbGlnaW9uIGhhdGUgdHdlZXRzOjwvYj4gMCwgPGJyPiA8Yj4gVGhlIG51bWJlciBvZiBtaXNvZ3lueSB0d2VldHM6PC9iPiAwLjA=&quot; width=&quot;300&quot; style=&quot;border:none !important;&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;`)[0];\n",
       "                popup_00bcd8e53fc3c1a077b081ef0b3d73a1.setContent(i_frame_ef6aba99c21b474a383c6040010ffc99);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_c5871e50b7b1447114b71d4015ca165a.bindPopup(popup_00bcd8e53fc3c1a077b081ef0b3d73a1)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_460d8cc357d5cf0800323e80b9b3b715 = L.marker(\n",
       "                [25.6242618, 42.3528328],\n",
       "                {}\n",
       "            ).addTo(map_8808f79cb3c120870045aef59d332bf0);\n",
       "        \n",
       "    \n",
       "        var popup_4abc20aa36c1cd0ad2c65fd20c2ebff7 = L.popup({&quot;maxWidth&quot;: 300});\n",
       "\n",
       "        \n",
       "            \n",
       "                var i_frame_06fcac6854417a2e28b5975a85eb6987 = $(`&lt;iframe src=&quot;data:text/html;charset=utf-8;base64,CiAgICA8Yj4gTnVtYmVyIG9mIHR3ZWV0czogPC9iPiAzLCAgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIG9mZmVuc2l2ZSB0d2VldHM6PC9iPiAwLjAsIDxicj4gPGI+VGhlIG51bWJlciBvZiByYWNpc20gdHdlZXRzOjwvYj4gMC4wLCA8YnI+IDxiPlRoZSBudW1iZXIgb2YgdmVyYmFsIGFidXNlIHR3ZWV0czo8L2I+IDAuMCwgPGJyPiA8Yj5UaGUgbnVtYmVyIG9mIHJlbGlnaW9uIGhhdGUgdHdlZXRzOjwvYj4gMCwgPGJyPiA8Yj4gVGhlIG51bWJlciBvZiBtaXNvZ3lueSB0d2VldHM6PC9iPiAwLjA=&quot; width=&quot;300&quot; style=&quot;border:none !important;&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;`)[0];\n",
       "                popup_4abc20aa36c1cd0ad2c65fd20c2ebff7.setContent(i_frame_06fcac6854417a2e28b5975a85eb6987);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_460d8cc357d5cf0800323e80b9b3b715.bindPopup(popup_4abc20aa36c1cd0ad2c65fd20c2ebff7)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x1f91d21c3a0>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create map zoomed in on northern Africa and the Middle East\n",
    "m = folium.Map(location=[27, 48], zoom_start=4, tiles='OpenStreetMap')\n",
    "\n",
    "# Add a circe for each dialect with the number of tweets as radius\n",
    "for i in range(len(dialect_df) - 1):\n",
    "    # get the location\n",
    "    location = dialect_df['Location'][i]\n",
    "    # get the number of tweets\n",
    "    radius = dialect_df['Total text'][i] * 2\n",
    "\n",
    "    Iframe = folium.IFrame(html=f'<b> Number of tweets: </b> {dialect_df[\"Total text\"][i]},  <br> <b>The number of offensive tweets:</b> {dialect_df[\"Offensiveness\"][i]}, <br> <b>The number of racism tweets:</b> {dialect_df[\"Racism\"][i]}, <br> <b>The number of verbal abuse tweets:</b> {dialect_df[\"Verbal Abuse\"][i]}, <br> <b>The number of religion hate tweets:</b> {dialect_df[\"Religion Hate\"][i]}, <br> <b> The number of misogyny tweets:</b> {dialect_df[\"Misogyny\"][i]}', width=300, height=150)\n",
    "    popup = folium.Popup(Iframe, max_width=300)\n",
    "    \n",
    "    # add a Marker with a popup object\n",
    "    folium.Marker(location= location, popup= popup).add_to(m)\n",
    "\n",
    "\n",
    "# show the map\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96f786b0b969ce9dd83e03f57258e7a9a05f55dd8c5f36e6eecbeebc399786bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
