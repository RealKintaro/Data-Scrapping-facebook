{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piplene of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\makch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                            ّ    | # Tashdid\n",
    "                            َ    | # Fatha\n",
    "                            ً    | # Tanwin Fath\n",
    "                            ُ    | # Damma\n",
    "                            ٌ    | # Tanwin Damm\n",
    "                            ِ    | # Kasra\n",
    "                            ٍ    | # Tanwin Kasr\n",
    "                            ْ    | # Sukun\n",
    "                            ـ     # Tatwil/Kashida\n",
    "                        \"\"\", re.VERBOSE)\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations = arabic_punctuations + english_punctuations\n",
    "\n",
    "\n",
    "def remove_urls (text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emails(text):\n",
    "    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "# def remove_emoji(text):\n",
    "#     return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "def remove_emoji(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def normalization(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_sentence = [w for w in text.split() if not w in arabic_stopwords]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def cleaning_content(line):\n",
    "    if (isinstance(line, float)):\n",
    "        return None\n",
    "    line.replace('\\n', ' ')\n",
    "    line = remove_emails(line)\n",
    "    line = remove_urls(line)\n",
    "    line = remove_emoji(line)\n",
    "    nline = [w if '@' not in w else 'USERID' for w in line.split()]\n",
    "    line = ' '.join(nline)\n",
    "    line = line.replace('RT', '').replace('<LF>', '').replace('<br />','').replace('&quot;', '').replace('<url>', '').replace('USERID', '')\n",
    "\n",
    "\n",
    "    # add spaces between punc,\n",
    "    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations}))\n",
    "\n",
    "    # then remove punc,\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    line = line.translate(translator)\n",
    "\n",
    "    line = remove_stopwords(line)\n",
    "    line=remove_diacritics(normalization(line))\n",
    "\n",
    "    line = line.strip()\n",
    "    return line\n",
    "\n",
    "def hasDigits(s):\n",
    "    return any( 48 <= ord(char) <= 57  or 1632 <= ord(char) <= 1641 for char in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offensive detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        #  hidden size of BERT, hidden size of our classifier, number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                        max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                        information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                        num_labels)\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)  \n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task and feed them to classifier to compute logits \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dictionary\n",
    "\n",
    "with open('../models/offensive_dict.pkl', 'rb') as handle:\n",
    "    off_dictionary = pickle.load(handle)\n",
    "\n",
    "off_model = BertClassifier()\n",
    "off_model.load_state_dict(torch.load('../models/modelv3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN =  256\n",
    "\n",
    "# Load MAX_LEN\n",
    "with open('../models/offensive_max_len.pkl', 'rb') as handle:\n",
    "    MAX_LEN = pickle.load(handle)\n",
    "\n",
    "\n",
    "def predict_off(review_text,model,device,tokenizer):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='longest',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1)\n",
    "    #print(f'Review text: {review_text}')\n",
    "    index = output.cpu().data.numpy().argmax()\n",
    "    #print(f'Sentiment  : {index}')\n",
    "    # decode the output of the model to get the predicted label\n",
    "    pred = off_dictionary[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Racism detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "racism_model = load_model('../models/racism/Racism_Detector.h5')\n",
    "\n",
    "# load tokenizer\n",
    "with open('../models/racism/racismtokenizer.pickle', 'rb') as handle:\n",
    "    rasicmtokinzer = pickle.load(handle)\n",
    "\n",
    "# load max_len\n",
    "with open('../models/racism/racismmaxlen.pickle', 'rb') as handle:\n",
    "    racism_max_len = pickle.load(handle)\n",
    "\n",
    "# load dictionary\n",
    "with open('../models/racism/racism_dict.pickle', 'rb') as handle:\n",
    "    racism_dict = pickle.load(handle)\n",
    "\n",
    "# predict\n",
    "def predict_racism(t):\n",
    "    # tokenize\n",
    "    t = rasicmtokinzer.texts_to_sequences(t)\n",
    "    # pad\n",
    "    t = pad_sequences(t, maxlen=racism_max_len)\n",
    "    # predict\n",
    "    p = racism_model.predict(t)\n",
    "    # decode\n",
    "    if p > 0.5:\n",
    "        p = racism_dict[1]\n",
    "    else:\n",
    "        p = racism_dict[0]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Religion Hate detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbal Abuse detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misogony detection model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "\n",
    "def predict(text):\n",
    "    # clean text\n",
    "    text = cleaning_content(text)\n",
    "    \n",
    "    # predict using offensive model\n",
    "    off_pred = predict_off(text,off_model,device,tokenizer)\n",
    "\n",
    "    if off_pred == 'offensive':\n",
    "        # predict using racism model\n",
    "        rac_pred = predict_racism([text])\n",
    "        return off_pred,rac_pred\n",
    "\n",
    "    # return the prediction\n",
    "    return off_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non_offensive'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "text = 'كل العرب يحبون السعودية'\n",
    "\n",
    "predict(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('offensive', 'Racism')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'اذهب لبلدك يا اسود أنت و شعبك المعفن'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non_offensive'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'هنالك قط اسود في المنزل'\n",
    "\n",
    "# predict\n",
    "predict(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96f786b0b969ce9dd83e03f57258e7a9a05f55dd8c5f36e6eecbeebc399786bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
