{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"DOCaTMFLZZpC"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nimport random\nimport time\nimport gc\nimport pickle\n\nimport nltk\nnltk.download('stopwords')\nimport emoji\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:25.093406Z","iopub.execute_input":"2022-12-12T03:06:25.094641Z","iopub.status.idle":"2022-12-12T03:06:28.614473Z","shell.execute_reply.started":"2022-12-12T03:06:25.094557Z","shell.execute_reply":"2022-12-12T03:06:28.613408Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:31.608368Z","iopub.execute_input":"2022-12-12T03:06:31.608805Z","iopub.status.idle":"2022-12-12T03:06:31.694821Z","shell.execute_reply.started":"2022-12-12T03:06:31.608770Z","shell.execute_reply":"2022-12-12T03:06:31.693581Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(torch.cuda.get_device_name(device))","metadata":{"id":"cKNYp_ZoN8Jq","outputId":"4a99b4b2-32e0-49a3-8a3a-50265528c7f2","execution":{"iopub.status.busy":"2022-12-12T03:06:32.693953Z","iopub.execute_input":"2022-12-12T03:06:32.694365Z","iopub.status.idle":"2022-12-12T03:06:32.702450Z","shell.execute_reply.started":"2022-12-12T03:06:32.694329Z","shell.execute_reply":"2022-12-12T03:06:32.701165Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preparation and cleaning\n","metadata":{"id":"oYZxEOZ0ZUsA"}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/final_dataset.csv', encoding='utf-16')\n#df.columns = ['Comment', 'Majority_Label']\ndf.head()\n","metadata":{"id":"Z3S7hzZfOASV","outputId":"28d27939-2f96-45f3-b000-60d72a133842","execution":{"iopub.status.busy":"2022-12-12T03:06:35.545695Z","iopub.execute_input":"2022-12-12T03:06:35.546656Z","iopub.status.idle":"2022-12-12T03:06:35.645801Z","shell.execute_reply.started":"2022-12-12T03:06:35.546606Z","shell.execute_reply":"2022-12-12T03:06:35.644636Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                             Comment  Majority_Label\n0  وزير الخارجيه اللبناني جبران باسيل قال سلسله ت...             0.0\n1          سوريه بلد الحضارات تربطها بعليه او بحيوان             0.0\n2  اخي الحاج اذا شعرت انك محرجا الانتقادات لتصريح...             0.0\n3  فيك تعيش بلا تكب فتن ليل نهار وبكره قلهم الموض...             0.0\n4   البطل قاتل وجاذف بحياته لتحيا انت واطي عيب الشوم             1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Majority_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>وزير الخارجيه اللبناني جبران باسيل قال سلسله ت...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>سوريه بلد الحضارات تربطها بعليه او بحيوان</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>اخي الحاج اذا شعرت انك محرجا الانتقادات لتصريح...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>فيك تعيش بلا تكب فتن ليل نهار وبكره قلهم الموض...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>البطل قاتل وجاذف بحياته لتحيا انت واطي عيب الشوم</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape[0] - df.dropna().shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:46.701298Z","iopub.execute_input":"2022-12-12T03:06:46.702391Z","iopub.status.idle":"2022-12-12T03:06:46.722735Z","shell.execute_reply.started":"2022-12-12T03:06:46.702351Z","shell.execute_reply":"2022-12-12T03:06:46.721806Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"df=df.dropna()\n# sorting by first name\ndf.sort_values(\"Comment\", inplace=True)\n# dropping ALL duplicate values\ndf.drop_duplicates(subset=\"Comment\",\n                     keep=False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:49.925195Z","iopub.execute_input":"2022-12-12T03:06:49.925594Z","iopub.status.idle":"2022-12-12T03:06:49.992137Z","shell.execute_reply.started":"2022-12-12T03:06:49.925563Z","shell.execute_reply":"2022-12-12T03:06:49.991166Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n\narabic_diacritics = re.compile(\"\"\"\n                             ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n\narabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\nenglish_punctuations = string.punctuation\npunctuations = arabic_punctuations + english_punctuations\n\n\ndef remove_urls (text):\n    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    return text\n\n\ndef remove_emails(text):\n    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n    return text\n\n# def remove_emoji(text):\n#     return emoji.get_emoji_regexp().sub(u'', text)\n\ndef remove_emoji(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\ndef normalization(text):\n    text = re.sub(\"[إأآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    text = re.sub(\"گ\", \"ك\", text)\n    return text\n\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', text)\n    return text\n\ndef remove_stopwords(text):\n    filtered_sentence = [w for w in text.split() if not w in arabic_stopwords]\n    return ' '.join(filtered_sentence)\n\ndef cleaning_content(line):\n    if (isinstance(line, float)):\n        return None\n    line.replace('\\n', ' ')\n    line = remove_emails(line)\n    line = remove_urls(line)\n    line = remove_emoji(line)\n    nline = [w if '@' not in w else 'USERID' for w in line.split()]\n    line = ' '.join(nline)\n    line = line.replace('RT', '').replace('<LF>', '').replace('<br />','').replace('&quot;', '').replace('<url>', '').replace('USERID', '')\n\n\n    # add spaces between punc,\n    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations}))\n\n    # then remove punc,\n    translator = str.maketrans('', '', punctuations)\n    line = line.translate(translator)\n\n    line = remove_stopwords(line)\n    line=remove_diacritics(normalization(line))\n    return line\n\ndef hasDigits(s):\n    return any( 48 <= ord(char) <= 57  or 1632 <= ord(char) <= 1641 for char in s)\n","metadata":{"id":"G8qBDZAUOvOz","execution":{"iopub.status.busy":"2022-12-12T03:06:51.693932Z","iopub.execute_input":"2022-12-12T03:06:51.694365Z","iopub.status.idle":"2022-12-12T03:06:51.714678Z","shell.execute_reply.started":"2022-12-12T03:06:51.694330Z","shell.execute_reply":"2022-12-12T03:06:51.713624Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.Comment = df.Comment.apply(cleaning_content)","metadata":{"id":"C9cVYayEQKP_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments = ' '.join(list(df.Comment))\nwords = comments.split(' ')","metadata":{"id":"1k6n4gnAQKJm","execution":{"iopub.status.busy":"2022-12-12T03:06:55.297588Z","iopub.execute_input":"2022-12-12T03:06:55.297966Z","iopub.status.idle":"2022-12-12T03:06:55.352762Z","shell.execute_reply.started":"2022-12-12T03:06:55.297934Z","shell.execute_reply":"2022-12-12T03:06:55.351788Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(df.Comment)\ncorpus[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:58.632660Z","iopub.execute_input":"2022-12-12T03:06:58.633069Z","iopub.status.idle":"2022-12-12T03:06:58.738203Z","shell.execute_reply.started":"2022-12-12T03:06:58.633037Z","shell.execute_reply":"2022-12-12T03:06:58.737124Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['<url>',\n 'إلى',\n 'اتخض',\n 'زى',\n 'لايك😂😂',\n '<url>',\n 'الله',\n 'غالبمن',\n 'الاردن',\n 'للوطن']"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:07:00.722606Z","iopub.execute_input":"2022-12-12T03:07:00.723145Z","iopub.status.idle":"2022-12-12T03:07:00.814929Z","shell.execute_reply.started":"2022-12-12T03:07:00.723111Z","shell.execute_reply":"2022-12-12T03:07:00.814017Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'الله': 4602,\n 'من': 4175,\n 'و': 3567,\n 'في': 3082,\n 'على': 2426,\n 'لا': 1471,\n 'ما': 1121,\n 'ان': 1066,\n 'يا': 1032,\n 'انت': 982}"},"metadata":{}}]},{"cell_type":"code","source":"X = df.Comment.values\nY = df.Majority_Label.values\n\nX_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.1, random_state = random.seed(42))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:07:10.109871Z","iopub.execute_input":"2022-12-12T03:07:10.110263Z","iopub.status.idle":"2022-12-12T03:07:10.118045Z","shell.execute_reply.started":"2022-12-12T03:07:10.110207Z","shell.execute_reply":"2022-12-12T03:07:10.116933Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02', do_lower_case=True)\n\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    input_ids = []\n    attention_masks = []\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=cleaning_content(sent),  \n            add_special_tokens=True,        \n            max_length=MAX_LEN,             \n            pad_to_max_length=True,         \n            return_attention_mask=True      \n        )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:07:17.946958Z","iopub.execute_input":"2022-12-12T03:07:17.947469Z","iopub.status.idle":"2022-12-12T03:07:18.931586Z","shell.execute_reply.started":"2022-12-12T03:07:17.947434Z","shell.execute_reply":"2022-12-12T03:07:18.930601Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MAX_LEN =  256\n\ntoken_ids = list(preprocessing_for_bert([X_train[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"id":"5tfA9o2URljj","outputId":"35097edf-cd0c-46f0-b4f4-d64ef48134da","execution":{"iopub.status.busy":"2022-12-12T03:07:22.627066Z","iopub.execute_input":"2022-12-12T03:07:22.627719Z","iopub.status.idle":"2022-12-12T03:07:39.601854Z","shell.execute_reply.started":"2022-12-12T03:07:22.627680Z","shell.execute_reply":"2022-12-12T03:07:39.600806Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Original:   <url> إلى اتخض زى لايك😂😂 \nToken IDs:  [2, 7252, 195, 678, 50261, 181, 39605, 7375, 1330, 1449, 21830, 15704, 2396, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTokenizing data...\n","output_type":"stream"}]},{"cell_type":"code","source":"Y_train","metadata":{"id":"WL0IIKADR7ft","outputId":"65e495c8-cd08-4c41-c9b8-b7edf3ecc316","execution":{"iopub.status.busy":"2022-12-12T03:08:24.335953Z","iopub.execute_input":"2022-12-12T03:08:24.336373Z","iopub.status.idle":"2022-12-12T03:08:24.344173Z","shell.execute_reply.started":"2022-12-12T03:08:24.336337Z","shell.execute_reply":"2022-12-12T03:08:24.342899Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([1., 1., 1., ..., 0., 1., 0.])"},"metadata":{}}]},{"cell_type":"code","source":"train_labels = torch.tensor(Y_train.astype(float))\nval_labels = torch.tensor(Y_val.astype(float))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:08:26.366537Z","iopub.execute_input":"2022-12-12T03:08:26.367549Z","iopub.status.idle":"2022-12-12T03:08:26.373803Z","shell.execute_reply.started":"2022-12-12T03:08:26.367500Z","shell.execute_reply":"2022-12-12T03:08:26.372466Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:08:57.209107Z","iopub.execute_input":"2022-12-12T03:08:57.209520Z","iopub.status.idle":"2022-12-12T03:08:57.217540Z","shell.execute_reply.started":"2022-12-12T03:08:57.209487Z","shell.execute_reply":"2022-12-12T03:08:57.216204Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\n\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        #  hidden size of BERT, hidden size of our classifier, number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)  \n        \n        # Extract the last hidden state of the token `[CLS]` for classification task and feed them to classifier to compute logits \n        last_hidden_state_cls = outputs[0][:, 0, :]\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"id":"9DSY4rPeSD-A","outputId":"717f451f-6569-4317-9e41-a71cceccd3b4","execution":{"iopub.status.busy":"2022-12-12T03:08:59.855854Z","iopub.execute_input":"2022-12-12T03:08:59.856246Z","iopub.status.idle":"2022-12-12T03:08:59.865330Z","shell.execute_reply.started":"2022-12-12T03:08:59.856200Z","shell.execute_reply":"2022-12-12T03:08:59.864266Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"CPU times: user 26 µs, sys: 1 µs, total: 27 µs\nWall time: 31 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"def initialize_model(epochs=3):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    bert_classifier.to(device)\n\n    optimizer = torch.optim.AdamW(bert_classifier.parameters(),lr=0.0001,eps=1e-8)\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"id":"iDyHb2_zSFzn","execution":{"iopub.status.busy":"2022-12-12T03:09:02.160714Z","iopub.execute_input":"2022-12-12T03:09:02.161093Z","iopub.status.idle":"2022-12-12T03:09:02.169197Z","shell.execute_reply.started":"2022-12-12T03:09:02.161061Z","shell.execute_reply":"2022-12-12T03:09:02.168202Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n            \n            # Always clear any previously calculated gradients before performing a\n            # backward pass. PyTorch doesn't do this automatically because \n            # accumulating the gradients is \"convenient while training RN\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n            \n            # Accumulate the training loss over all of the batches so that we can\n            # calculate the average loss at the end. `loss` is a Tensor containing a\n            # single value; the `.item()` function just returns the Python value \n            # from the tensor.\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels.long())\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n        \n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n        \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels.long())\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:09:05.008157Z","iopub.execute_input":"2022-12-12T03:09:05.008766Z","iopub.status.idle":"2022-12-12T03:09:05.028650Z","shell.execute_reply.started":"2022-12-12T03:09:05.008731Z","shell.execute_reply":"2022-12-12T03:09:05.027685Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()   \n\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=5)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)","metadata":{"id":"8WXZh9TdSLkz","outputId":"05c4a182-7f5d-4d1d-a1a3-074f1296711b","execution":{"iopub.status.busy":"2022-12-12T03:09:12.559440Z","iopub.execute_input":"2022-12-12T03:09:12.559854Z","iopub.status.idle":"2022-12-12T04:18:03.516320Z","shell.execute_reply.started":"2022-12-12T03:09:12.559822Z","shell.execute_reply":"2022-12-12T04:18:03.515187Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Start training...\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   1    |   20    |   0.632697   |     -      |     -     |   25.70  \n   1    |   40    |   0.575155   |     -      |     -     |   24.67  \n   1    |   60    |   0.567682   |     -      |     -     |   25.44  \n   1    |   80    |   0.532807   |     -      |     -     |   26.46  \n   1    |   100   |   0.530171   |     -      |     -     |   26.30  \n   1    |   120   |   0.532441   |     -      |     -     |   25.95  \n   1    |   140   |   0.483929   |     -      |     -     |   26.07  \n   1    |   160   |   0.501306   |     -      |     -     |   26.23  \n   1    |   180   |   0.459289   |     -      |     -     |   26.19  \n   1    |   200   |   0.480510   |     -      |     -     |   26.18  \n   1    |   220   |   0.493092   |     -      |     -     |   26.17  \n   1    |   240   |   0.441885   |     -      |     -     |   26.17  \n   1    |   260   |   0.440011   |     -      |     -     |   26.15  \n   1    |   280   |   0.453355   |     -      |     -     |   26.15  \n   1    |   300   |   0.469530   |     -      |     -     |   26.16  \n   1    |   320   |   0.444506   |     -      |     -     |   26.16  \n   1    |   340   |   0.438919   |     -      |     -     |   26.15  \n   1    |   360   |   0.431392   |     -      |     -     |   26.14  \n   1    |   380   |   0.407238   |     -      |     -     |   26.15  \n   1    |   400   |   0.406337   |     -      |     -     |   26.13  \n   1    |   420   |   0.462693   |     -      |     -     |   26.15  \n   1    |   440   |   0.455140   |     -      |     -     |   26.12  \n   1    |   460   |   0.399589   |     -      |     -     |   26.14  \n   1    |   480   |   0.416999   |     -      |     -     |   26.12  \n   1    |   500   |   0.427660   |     -      |     -     |   26.12  \n   1    |   520   |   0.422805   |     -      |     -     |   26.12  \n   1    |   540   |   0.425475   |     -      |     -     |   26.13  \n   1    |   560   |   0.437308   |     -      |     -     |   26.14  \n   1    |   580   |   0.443980   |     -      |     -     |   26.15  \n   1    |   600   |   0.417805   |     -      |     -     |   26.15  \n   1    |   606   |   0.457651   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   1    |    -    |   0.467896   |  0.381081  |   82.69   |  822.69  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   2    |   20    |   0.275117   |     -      |     -     |   27.56  \n   2    |   40    |   0.295583   |     -      |     -     |   26.20  \n   2    |   60    |   0.248752   |     -      |     -     |   26.15  \n   2    |   80    |   0.325117   |     -      |     -     |   26.20  \n   2    |   100   |   0.304514   |     -      |     -     |   26.22  \n   2    |   120   |   0.289015   |     -      |     -     |   26.16  \n   2    |   140   |   0.352266   |     -      |     -     |   26.13  \n   2    |   160   |   0.237176   |     -      |     -     |   26.12  \n   2    |   180   |   0.285141   |     -      |     -     |   26.12  \n   2    |   200   |   0.257377   |     -      |     -     |   26.06  \n   2    |   220   |   0.279229   |     -      |     -     |   26.08  \n   2    |   240   |   0.308591   |     -      |     -     |   26.13  \n   2    |   260   |   0.316307   |     -      |     -     |   26.10  \n   2    |   280   |   0.263285   |     -      |     -     |   26.13  \n   2    |   300   |   0.279234   |     -      |     -     |   26.12  \n   2    |   320   |   0.273642   |     -      |     -     |   26.12  \n   2    |   340   |   0.269718   |     -      |     -     |   26.09  \n   2    |   360   |   0.289498   |     -      |     -     |   26.11  \n   2    |   380   |   0.256761   |     -      |     -     |   26.10  \n   2    |   400   |   0.266357   |     -      |     -     |   26.12  \n   2    |   420   |   0.253105   |     -      |     -     |   26.15  \n   2    |   440   |   0.271702   |     -      |     -     |   26.14  \n   2    |   460   |   0.265906   |     -      |     -     |   26.15  \n   2    |   480   |   0.266736   |     -      |     -     |   26.13  \n   2    |   500   |   0.279243   |     -      |     -     |   26.16  \n   2    |   520   |   0.268492   |     -      |     -     |   26.16  \n   2    |   540   |   0.268468   |     -      |     -     |   26.14  \n   2    |   560   |   0.283312   |     -      |     -     |   26.06  \n   2    |   580   |   0.289147   |     -      |     -     |   26.21  \n   2    |   600   |   0.256276   |     -      |     -     |   26.25  \n   2    |   606   |   0.249149   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   2    |    -    |   0.278866   |  0.375432  |   84.44   |  826.16  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   3    |   20    |   0.131200   |     -      |     -     |   27.48  \n   3    |   40    |   0.158962   |     -      |     -     |   26.24  \n   3    |   60    |   0.148332   |     -      |     -     |   26.13  \n   3    |   80    |   0.163261   |     -      |     -     |   26.09  \n   3    |   100   |   0.118509   |     -      |     -     |   26.06  \n   3    |   120   |   0.215144   |     -      |     -     |   26.08  \n   3    |   140   |   0.185493   |     -      |     -     |   26.12  \n   3    |   160   |   0.164525   |     -      |     -     |   26.13  \n   3    |   180   |   0.128894   |     -      |     -     |   26.14  \n   3    |   200   |   0.207446   |     -      |     -     |   26.17  \n   3    |   220   |   0.169355   |     -      |     -     |   26.14  \n   3    |   240   |   0.152911   |     -      |     -     |   26.17  \n   3    |   260   |   0.177090   |     -      |     -     |   26.19  \n   3    |   280   |   0.170668   |     -      |     -     |   26.14  \n   3    |   300   |   0.135046   |     -      |     -     |   26.11  \n   3    |   320   |   0.120991   |     -      |     -     |   26.09  \n   3    |   340   |   0.111832   |     -      |     -     |   26.07  \n   3    |   360   |   0.132593   |     -      |     -     |   26.12  \n   3    |   380   |   0.190670   |     -      |     -     |   26.13  \n   3    |   400   |   0.116443   |     -      |     -     |   26.09  \n   3    |   420   |   0.145945   |     -      |     -     |   26.13  \n   3    |   440   |   0.153953   |     -      |     -     |   26.10  \n   3    |   460   |   0.167467   |     -      |     -     |   26.11  \n   3    |   480   |   0.154166   |     -      |     -     |   26.09  \n   3    |   500   |   0.172910   |     -      |     -     |   26.07  \n   3    |   520   |   0.145143   |     -      |     -     |   26.03  \n   3    |   540   |   0.137809   |     -      |     -     |   26.15  \n   3    |   560   |   0.134882   |     -      |     -     |   26.21  \n   3    |   580   |   0.212680   |     -      |     -     |   26.22  \n   3    |   600   |   0.164158   |     -      |     -     |   26.20  \n   3    |   606   |   0.111170   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   3    |    -    |   0.155795   |  0.448201  |   84.76   |  825.79  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   4    |   20    |   0.079654   |     -      |     -     |   27.42  \n   4    |   40    |   0.103250   |     -      |     -     |   26.20  \n   4    |   60    |   0.074128   |     -      |     -     |   26.22  \n   4    |   80    |   0.082321   |     -      |     -     |   26.09  \n   4    |   100   |   0.050973   |     -      |     -     |   26.03  \n   4    |   120   |   0.030560   |     -      |     -     |   26.11  \n   4    |   140   |   0.057613   |     -      |     -     |   26.15  \n   4    |   160   |   0.063887   |     -      |     -     |   26.16  \n   4    |   180   |   0.110825   |     -      |     -     |   26.17  \n   4    |   200   |   0.114845   |     -      |     -     |   26.17  \n   4    |   220   |   0.054168   |     -      |     -     |   26.15  \n   4    |   240   |   0.070327   |     -      |     -     |   26.15  \n   4    |   260   |   0.102828   |     -      |     -     |   26.17  \n   4    |   280   |   0.064933   |     -      |     -     |   26.11  \n   4    |   300   |   0.065186   |     -      |     -     |   26.07  \n   4    |   320   |   0.124393   |     -      |     -     |   26.08  \n   4    |   340   |   0.078961   |     -      |     -     |   26.09  \n   4    |   360   |   0.092712   |     -      |     -     |   26.10  \n   4    |   380   |   0.074000   |     -      |     -     |   26.06  \n   4    |   400   |   0.110029   |     -      |     -     |   26.07  \n   4    |   420   |   0.094902   |     -      |     -     |   26.11  \n   4    |   440   |   0.080456   |     -      |     -     |   26.11  \n   4    |   460   |   0.053258   |     -      |     -     |   26.05  \n   4    |   480   |   0.110463   |     -      |     -     |   26.08  \n   4    |   500   |   0.095672   |     -      |     -     |   26.18  \n   4    |   520   |   0.112931   |     -      |     -     |   26.24  \n   4    |   540   |   0.103869   |     -      |     -     |   26.14  \n   4    |   560   |   0.055404   |     -      |     -     |   26.10  \n   4    |   580   |   0.097936   |     -      |     -     |   26.10  \n   4    |   600   |   0.052078   |     -      |     -     |   26.16  \n   4    |   606   |   0.057331   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   4    |    -    |   0.081837   |  0.704816  |   84.81   |  825.62  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   5    |   20    |   0.018114   |     -      |     -     |   27.46  \n   5    |   40    |   0.028991   |     -      |     -     |   26.10  \n   5    |   60    |   0.049602   |     -      |     -     |   26.07  \n   5    |   80    |   0.049008   |     -      |     -     |   26.05  \n   5    |   100   |   0.053812   |     -      |     -     |   26.03  \n   5    |   120   |   0.072620   |     -      |     -     |   26.08  \n   5    |   140   |   0.025770   |     -      |     -     |   26.10  \n   5    |   160   |   0.069165   |     -      |     -     |   26.11  \n   5    |   180   |   0.066999   |     -      |     -     |   26.09  \n   5    |   200   |   0.073383   |     -      |     -     |   26.10  \n   5    |   220   |   0.055811   |     -      |     -     |   26.10  \n   5    |   240   |   0.075456   |     -      |     -     |   26.09  \n   5    |   260   |   0.088140   |     -      |     -     |   26.07  \n   5    |   280   |   0.028008   |     -      |     -     |   26.10  \n   5    |   300   |   0.068750   |     -      |     -     |   26.15  \n   5    |   320   |   0.059598   |     -      |     -     |   26.15  \n   5    |   340   |   0.045083   |     -      |     -     |   26.16  \n   5    |   360   |   0.034492   |     -      |     -     |   26.20  \n   5    |   380   |   0.076379   |     -      |     -     |   26.18  \n   5    |   400   |   0.035995   |     -      |     -     |   26.20  \n   5    |   420   |   0.039004   |     -      |     -     |   26.17  \n   5    |   440   |   0.025841   |     -      |     -     |   26.19  \n   5    |   460   |   0.046406   |     -      |     -     |   26.06  \n   5    |   480   |   0.045192   |     -      |     -     |   26.12  \n   5    |   500   |   0.026545   |     -      |     -     |   26.24  \n   5    |   520   |   0.029535   |     -      |     -     |   26.22  \n   5    |   540   |   0.045082   |     -      |     -     |   26.23  \n   5    |   560   |   0.046840   |     -      |     -     |   26.17  \n   5    |   580   |   0.028203   |     -      |     -     |   26.12  \n   5    |   600   |   0.051803   |     -      |     -     |   26.11  \n   5    |   606   |   0.030778   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   5    |    -    |   0.048427   |  0.730894  |   84.95   |  825.73  \n----------------------------------------------------------------------\n\n\nTraining complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"def bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"id":"ydTaXNbZgcGI","execution":{"iopub.status.busy":"2022-12-12T04:18:41.668291Z","iopub.execute_input":"2022-12-12T04:18:41.668902Z","iopub.status.idle":"2022-12-12T04:18:41.676243Z","shell.execute_reply.started":"2022-12-12T04:18:41.668863Z","shell.execute_reply":"2022-12-12T04:18:41.675118Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"torch.save(bert_classifier.state_dict(), \"/kaggle/working/modelv3.pt\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:18:47.428759Z","iopub.execute_input":"2022-12-12T04:18:47.429118Z","iopub.status.idle":"2022-12-12T04:18:48.670644Z","shell.execute_reply.started":"2022-12-12T04:18:47.429086Z","shell.execute_reply":"2022-12-12T04:18:48.669611Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = BertClassifier()\nmodel.load_state_dict(torch.load(\"/kaggle/working/modelv1.pt\"))\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"id":"7kF8Uz_dg1M-","execution":{"iopub.status.busy":"2022-12-12T04:19:13.244601Z","iopub.execute_input":"2022-12-12T04:19:13.244974Z","iopub.status.idle":"2022-12-12T04:19:13.253631Z","shell.execute_reply.started":"2022-12-12T04:19:13.244941Z","shell.execute_reply":"2022-12-12T04:19:13.252394Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"probs = bert_predict(bert_classifier, val_dataloader)\n\n# Evaluate the Bert classifier\nevaluate_roc(probs, Y_val.astype(float))","metadata":{"id":"tAcb4H35hv7r","outputId":"b0133587-6829-4790-f4c7-eca76d876a2d","execution":{"iopub.status.busy":"2022-12-12T04:19:18.681164Z","iopub.execute_input":"2022-12-12T04:19:18.682143Z","iopub.status.idle":"2022-12-12T04:19:52.071281Z","shell.execute_reply.started":"2022-12-12T04:19:18.682096Z","shell.execute_reply":"2022-12-12T04:19:52.070269Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"AUC: 0.9164\nAccuracy: 84.94%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6pklEQVR4nO3dd3hUZfbA8e8BaQIiAmuhiQpKESn50WzYAVFUEMGKoqjYRVd3dVdl3XUtixULiIu6CqvYsCCs0hRFivSmCAJBEUQQEAIknN8f58YMIZlMysydSc7neeaZcu/cOXOTmTP3vu97XlFVnHPOufyUCzsA55xzyc0ThXPOuag8UTjnnIvKE4VzzrmoPFE455yLyhOFc865qDxRuEIRkUUi0jnsOJKFiPxZRF4M6bVHisiDYbx2SRORS0RkQhGf6/+TceaJIoWJyPciskNEtonIuuCLo1o8X1NVm6vq5Hi+RjYRqSQiD4nI6uB9fisid4qIJOL184ins4ikRz6mqv9Q1avj9HoiIjeLyEIR+U1E0kXkTRE5Nh6vV1Qicr+I/Kc421DV11T1zBhea5/kmMj/ybLKE0XqO0dVqwGtgNbAn8INp/BEZL98Fr0JnAZ0A6oDlwEDgCfjEIOISLJ9Hp4EbgFuBg4CmgDvAmeX9AtF+RvEXZiv7WKkqn5J0QvwPXB6xP1HgA8j7ncAvgA2A/OAzhHLDgL+DfwAbALejVjWHZgbPO8LoGXu1wQOA3YAB0Usaw38DFQI7l8FLAm2Px5oGLGuAjcA3wIr83hvpwEZQP1cj7cHsoCjgvuTgYeAGcAW4L1cMUXbB5OBvwPTgvdyFHBlEPNWYAVwbbBu1WCdPcC24HIYcD/wn2Cdw4P3dQWwOtgX90S8XhXg5WB/LAH+CKTn87dtHLzPdlH+/iOBocCHQbxfAUdGLH8SWBPsl9nAiRHL7gfGAP8Jll8NtAO+DPbVj8AzQMWI5zQH/gf8AvwE/BnoAuwCdgf7ZF6wbg1gRLCdtcCDQPlgWb9gnz8ObAyW9QM+D5ZLsGx9ENsCoAX2I2F38HrbgPdzfw6A8kFc3wX7ZDa5/of8UoTvmrAD8Esx/nh7f0DqBR+oJ4P7dYMPYTfsyPGM4H6dYPmHwH+BmkAF4OTg8dbBB7R98KG7InidSnm85kTgmoh4HgWeD273AJYDTYH9gHuBLyLW1eBL5yCgSh7v7Z/AlHze9ypyvsAnB19ELbAv87fI+eIuaB9Mxr7QmwcxVsB+rR8ZfFmdDGwH2gTrdybXFzt5J4rhWFI4DtgJNI18T8E+rwfMz729iO1eB6wq4O8/Mng/7YL4XwNGRyy/FKgVLBsErAMqR8S9Gzgv2DdVgLZYYt0veC9LgFuD9atjX/qDgMrB/fa590HEa78DvBD8Tf6AJfLsv1k/IBO4KXitKuydKM7CvuAPDP4OTYFDI97zg1E+B3din4Ojg+ceB9QK+7Oa6pfQA/BLMf549gHZhv1yUuBT4MBg2V3Aq7nWH4998R+K/TKumcc2nwP+luuxZeQkksgP5dXAxOC2YL9eTwrujwP6R2yjHPal2zC4r8CpUd7bi5FfermWTSf4pY592f8zYlkz7Bdn+Wj7IOK5gwvYx+8CtwS3OxNboqgXsXwG0Ce4vQI4K2LZ1bm3F7HsHmB6AbGNBF6MuN8NWBpl/U3AcRFxTy1g+7cC7wS3+wJz8lnv930Q3D8YS5BVIh7rC0wKbvcDVufaRj9yEsWpwDdY0iqXx3uOliiWAT2K+9nyy96XZDsn6wrvPFWtjn2JHQPUDh5vCFwoIpuzL8AJWJKoD/yiqpvy2F5DYFCu59XHTrPk9hbQUUQOBU7Cks9nEdt5MmIbv2DJpG7E89dEeV8/B7Hm5dBgeV7bWYUdGdQm+j7IMwYR6Soi00Xkl2D9buTs01iti7i9HcjuYHBYrteL9v43kv/7j+W1EJE7RGSJiPwavJca7P1ecr/3JiLyQdAxYgvwj4j162Onc2LREPsb/Bix31/AjizyfO1IqjoRO+01FFgvIsNE5IAYX7swcboYeaIoJVR1CvZr67HgoTXYr+kDIy5VVfWfwbKDROTAPDa1Bvh7ruftr6qj8njNTcAE4CLgYuwIQCO2c22u7VRR1S8iNxHlLX0CtBeR+pEPikh77MtgYsTDkes0wE6p/FzAPtgnBhGphCW/x4CDVfVA4CMswRUUbyx+xE455RV3bp8C9UQkrSgvJCInYm0gvbEjxwOBX8l5L7Dv+3kOWAo0VtUDsHP92euvAY7I5+Vyb2cNdkRRO2K/H6CqzaM8Z+8Nqj6lqm2xI8Qm2CmlAp8XvPaRBazjCskTRenyBHCGiByHNVKeIyJniUh5EakcdO+sp6o/YqeGnhWRmiJSQUROCrYxHLhORNoHPYGqisjZIlI9n9d8Hbgc6BXczvY88CcRaQ4gIjVE5MJY34iqfoJ9Wb4lIs2D99AheF/Pqeq3EatfKiLNRGR/YDAwRlWzou2DfF62IlAJ2ABkikhXILLL5k9ALRGpEev7yOUNbJ/UFJG6wI35rRi8v2eBUUHMFYP4+4jI3TG8VnWsHWADsJ+I/BUo6Fd5dazxeJuIHANcH7HsA+BQEbk16LZcPUjaYPvl8OxeY8H/1wTgXyJygIiUE5EjReTkGOJGRP4v+P+rAPyGdWrYE/Fa+SUssFOWfxORxsH/b0sRqRXL67r8eaIoRVR1A/AK8FdVXYM1KP8Z+7JYg/0qy/6bX4b98l6KNV7fGmxjFnANdui/CWuQ7hflZcdiPXTWqeq8iFjeAR4GRgenMRYCXQv5lnoCk4CPsbaY/2A9aW7Ktd6r2NHUOqyh9eYghoL2wV5UdWvw3Dew935x8P6yly8FRgErglMqeZ2Oi2YwkA6sxI6YxmC/vPNzMzmnYDZjp1TOB96P4bXGY/vtG+x0XAbRT3UB3IG9563YD4b/Zi8I9s0ZwDnYfv4WOCVY/GZwvVFEvg5uX44l3sXYvhxDbKfSwBLa8OB5q7DTcI8Gy0YAzYL9/24ezx2C/f0mYElvBNZY7opBcs4UOJd6RGQy1pAayujo4hCR67GG7ph+aTsXFj+icC5BRORQETk+OBVzNNbV9J2w43KuIHFLFCLykoisF5GF+SwXEXlKRJaLyHwRaROvWJxLEhWx3j9bscb497B2COeSWtxOPQWNo9uAV1S1RR7Lu2Hnmrthg7ueVNX2uddzzjkXrrgdUajqVKzvfH56YElEVXU6cGDQH98551wSCbMYV1327oWRHjz2Y+4VRWQAVueFqlWrtj3mmGMSEqBzzu3Zk3M7IwN++QXK5fETe/NmqFAh5/6WLSBil7y2lSgNWMWBbGY+mT+rap2ibCMlqjaq6jBgGEBaWprOmjUr5Iicc6li9WrYscNu79gBU6fCZ5/BQQflrPPbbzBuHBx66N5f7AvzbGGNrmVLu1aFzEw46aScZTt3wlFHQf1oQy1LgO5RGjWCQw8Tqr7yHOU2rufAIfevKur2wkwUa9l7ZGq94DHnXCmzYAH8HFF05dNPoUoVmD3bvrzLl8/7eRkZtu4BB8B+Rfi2+iXayW/gkEPsOivLvsTLlYPGjXOWN25sX/jHH2/3Ve2L/rzz8t5eODOl5LJ2LVx/PVx0EbS7BP4cjJsccn+RNxlmohgL3Cgio7HG7F+DEZ3OuRSiCh98AK+/nvMrfcEC2LABKlWCefOiPx+gbdv8t920KdSrB0cfXbT4fvkFzjor57RQ+fLQuTP84Q9Rn5Z6VOHFF+GOO2D3bji75KYtiVuiEJFRWKG62mKzgt2HFQpDVZ/Hauh0w0b+bsfmAXDOFcKePYk/771iBXz3HXz1FbzwAqxbt/fy2rXtlMvmzdCtGzRsaEcTt90GdYIz5BUqQJs29qVdvnze5/xdIXz3HVxzDUyaBKecAsOHw5ElV/IqbolCVfsWsFyxiWucK9NUYeJE+wWuCh9/DAceCD/9BDNnQs2aeZ/S2LUL5s9PeLh5uuACuPdeaN067EjKqAUL7DzesGFw9dUlfg4sJRqznUt1u3bBnDnWsLpwITz4oP26Ll8efvgh7+dUrWqNrG3awMEH571OrVrWMBp5Xj3edu2y00CNG1vDbeXKiXttF2HhQvj6a7j8cms0WbHC/iHiwBOFcyUgKyvnFMy331oPmszMnOVDhuz7nIMOymkk3bQJbrjBet1UqABHHJEkDaMu+ezaBf/4h10OPhh697ZsHackAZ4onItq7lx47z146CE7955f75zVq/N+vFowjVDFita+OG6cfbaPPtp6/ThXKF99Bf37w6JFcOml8PjjCTmk80ThyrSsLGtoXbAApk+3QVKZmfb5y61GDWjXLvq2TjzRGpcbNrT+8/vvH7/YXRmzdq39gx18sHUzK8FeTQXxROFKvS1brFMIwPPPw4QJ9qVeoYKd1s0t+6ihVi249lpo0gR69sw5OnAuob75xv4J69aF//4XTjvNBpYkkCcKV2ps22Y9hS65xI7Qwfrx78xjaqD997eeOp06WU+j//s/OO44aNXKehw5F7rNm+GPf7SxEZMn2yHq+eeHEoonCpeS0tPhnXfs3D/AM8/sW27h5JOhQwdLBFWrWiIoV866mfvRgUtqY8fa6Op16+DOO+2XTIg8UbiUsGsXvP029I06Ogcefti6nV5+ef4Nz84ltauvhhEj4NhjrSdFWlrYEXmicMlj1iwYPDintMIXX1jDcrlysGzZ3usOHAgtWsC55+aM6q1Z0/v0uxSVPS+QiCWGhg3hrrtyDplD5onCxV1mplXrnDfPCrzt3Ln3r/0NG2xQaaS6da3Bef16a0g+5hhLINdcE/pRuHMla80auO466NMHLrvMbicZTxSuRKnCypU5g83ef99qlOUW2c20XDnr1NGkiXUNv+iixMTqXKj27LFiWXfdZb+KQmqojoUnCldoWVnw1lvwyit7jx5esQIWL877OZUrw//+Z8mgTh0fdezKuG+/tbaIqVPh9NOtRlOjRmFHlS9PFC6q7dutG/e8efDIIza6+Ntvc5a3aZNzu1IlOOwwaN8eevXKefyEE6BBg8TF7FzSW7zYKjq+9BL065f0v5w8UbjfZWVZWejvvrO2gUqV9p5sJtvpp9sX/xVX7D17l3MuinnzrCbMFVdAjx52CF6zZthRxcQTRRm1bRuMHw/Tptmp0p9/htdey1les6b9L5crB9WrW5tC+/bWGSPJf/w4l1x27rRywf/8p1V9vOgiOxebIkkCPFGUSRdeCGPG7P1Ydk2iY46xHz2VKiU8LOdKny+/tCJ+S5bY4J4hQ1KyD7cnijIgK8sGen74oSWA7CRx0002+POIIzwxOFfi1q618gCHHAIffQRdu4YdUZF5oiilduywkhYzZsCNN+67/Ikn4JZbEh6Wc6XfkiU20XfduvDGG1bEr3r1sKMqFk8UpdBdd1kPpdyWLLF5ELyNwbk42LQJBg2Cf//bur2eeKLNPFcKeKIoRbLnQ/jyS7t/zTVW4qJtW2tDc87FyTvvWF2ZDRvgT38qdeUDPFGkuF27rH1syxY7nbRjhz0+erSPcHYuIa66yo4iWrWyhsDIwUWlhCeKFJaVlXcj9LZtVlbbORcnkUX8OnSAxo2tVk2FCuHGFSflwg7AFd6OHTbyuXHjnMd++cX+d7PnXnDOxcmqVdaD6dVX7f6AAXa6qZQmCfBEkRIyMuD1161+2KGH2piHt96y4nudO1sJ7hQau+NcatqzB4YOtfr2n39u9WzKCD/1lGSWLrXu12vWwKOP2kjoceP2XqdaNWukHjbMjx6cS4hly6yI3+efw5ln2q+2ww8PO6qE8USRRO69F/7+970fW73aei1VqGBHFZUrew8m5xJu2TJYtAhGjrQR1mWsj7knipAtWGBzpz/1VM5jw4ZZKY1DDtm7HcI5l0Bz5lg9myuvtEP4FSvgwAPDjioUnihCoGpHCpddZjO/ZatcGaZM2XtSH+dcgmVk2Jy8jzxio6v79rUPZxlNEuCN2aF46SU7vZmdJJ580trFduzwJOFcqKZNs/EQDz1kp5jmzk3JIn4lzY8oEuSHH2xU/+uv2/SgAC+/bLMfpngZGOdKh7Vr4ZRT7Chi/HhrtHaAJ4qEuP12ePzxvR+7/nr7weKcC9nixdCsmSWIt96yZFGtWthRJRU/9RQne/bAOedA69Y5SeLOO63jxO7d8Oyz4cbnXJn3yy82DWnz5na4D/ah9SSxDz+iKGF79sDNN9u4nGzt21tRyQsvDC8u51yEt96CG26AjRvhnnu8cbAAnihK2AMP5CSJKlWs8rBPCuRcEunXzxoI27SBjz+2xmsXlSeKEvLss9ZRIj3d7q9eDfXrhxuTcy4QWcSvUyebWGjQINjPvwJjEdc2ChHpIiLLRGS5iNydx/IGIjJJROaIyHwR6RbPeOLluefsKDY93Up7jxjhScK5pLFypfVgeuUVuz9ggM3u5UkiZnFLFCJSHhgKdAWaAX1FpFmu1e4F3lDV1kAfIGWaeDMyrNxLWprNVwLWaD16tJWnd86FLCvLSh60aAHTp+ccVbhCi2dKbQcsV9UVACIyGugBLI5YR4EDgts1gB/iGE+JmTrV5kyPNGoU9OkTTjzOuVyWLIH+/W26x65d4fnnoUGDsKNKWfFMFHWBNRH304H2uda5H5ggIjcBVYHT89qQiAwABgA0CPmPrQqnB1F27myjrOvVK9Wl6J1LPcuXWyG/V1+FSy4pc0X8SlrY4yj6AiNVtR7QDXhVRPaJSVWHqWqaqqbVqVMn4UFGWr3axkFUqwaTJkGjRp4knEsKs2fbLzew8RArV8Kll3qSKAHxTBRrgcgm3XrBY5H6A28AqOqXQGWgdhxjKrYOHez6iSdCDcM5l23HDrj7bhuw9Le/WQMiwAEHRH+ei1k8E8VMoLGINBKRilhj9dhc66wGTgMQkaZYotgQx5iKZcECWLfObnv5DeeSwNSpcNxx8PDDNj5izhwv4hcHcWujUNVMEbkRGA+UB15S1UUiMhiYpapjgUHAcBG5DWvY7qeafF0TVK3C8JYtdn/4cD/d5Fzo1q6F006zvuiffGK3XVxIEn4vR5WWlqazZs1K2Ovt2QPly+fcf/11K0/vnAvJggVw7LF2+4MPrIifzwlcIBGZrappRXlu2I3ZSUt132lxN23yJOFcaH7+2Wb7atkyp4hf9+6eJBLAhybm4/rrLVGAnWZKTy/TE1w5Fx5VePNNuPFG+7V2333WcO0SxhNFHt57LydJfPONz1vtXKiuuMLGQ6Slwaef5px2cgnjiSIPTz5p13/7mycJ50IRWcTv5JPtdNOtt3p9ppB4G0Ueduyw8RL33ht2JM6VQStWWPmDkSPtfv/+cMcdniRC5IkiwpYtNkZn+nQbfe2cS6CsLBvJeuyxMHMmlPOvp2ThfwksKYwZAzVqwNat9tgzz4Qbk3NlyuLFcPzxcNtt1t118WJrm3BJocwfy61bB4cemnO/Zk1Yv96Pcp1LqJUr4bvvbKBSnz5enynJlPkjim+/tesjjoClS22+dU8SziXAzJlW5gDg7LOtbaJvX08SSajMJ4psw4bB0UeHHYVzZcD27dY43aGDzR+cXcSvevVw43L5KtOJ4qef4KST7Lb/iHEuASZPtq6u//oXXHONF/FLEWX2JMvmzXDIIXb7qKN8oKdzcZeeDmecAQ0bwsSJ1mjtUkKZPaIYMMCujzjCZk30cjHOxcm8eXZdr56VPZg/35NEiimTieLrr610DFhjtjdeOxcHGzbAxRdDq1YwZYo91q0b7L9/qGG5witziWL0aGjb1m4PHuxjepwrcaowahQ0a2YDlB54ADp2DDsqVwxl6rf0+vU5ZcL79/cSHc7FxWWXwWuvWcPfiBHQvHnYEbliijlRiMj+qro9nsHE2+DBdn3ttfD88+HG4lypsmePdR0UsfaHtm3h5pv3nvXLpawCT7yISCcRWQwsDe4fJyLPxj2yOMiu3/Tcc+HG4Vypsny5TUP673/b/f79rRSHJ4lSI5Yz9I8DZwEbAVR1HnBSPIOKp0MO8TETzpWIzEx47DEr4jdnDlSsGHZELk5iOvWkqmtk72/XrPiE45xLCQsXwpVXwqxZ0KMHPPssHHZY2FG5OIklUawRkU6AikgF4BZgSXzDKnmqVqajTp2wI3GuFFi9Glatsm6EvXv7YXopF0uiuA54EqgLrAUmAAPjGVQ8rFhh1xs2hBuHcynrq69s8NyAATYeYsUKqFYt7KhcAsTSRnG0ql6iqger6h9U9VKgabwDK2m7dtn1qFHhxuFcyvntN7j9dhsL8cgjsHOnPe5JosyIJVE8HeNjSS07UfgRsnOFMHGiFfF7/HG47jora1CpUthRuQTL99STiHQEOgF1ROT2iEUHACnX7+2GG+za/8edi1F6Opx1FjRqZCU4TkrZzo6umKK1UVQEqgXrRBaK3wL0imdQJU0Vpk2z2+eeG24sziW9OXOgdWsr4vf++3DyyVClSthRuRDlmyhUdQowRURGquqqBMZU4j77zK4PO8xrOzmXr59+stHUb7xh80acfDJ06RJ2VC4JxNLrabuIPAo0B36fYURVT41bVCVs4kS7Hj063DicS0qqVpvplltg2zZ48EHo1CnsqFwSieX39WtY+Y5GwAPA98DMOMZU4oYNs+sWLcKNw7mkdPHFVsjv6KNh7ly45x6oUCHsqFwSieWIopaqjhCRWyJOR6VUoqhSxUrg16wZdiTOJYnIIn5nnmldX2+4weszuTzFckQRlNLjRxE5W0RaAwfFMaYS9e9/27ig884LOxLnksQ331iF15desvtXXumVXl1UsRxRPCgiNYBB2PiJA4Bb4xlUSbrqKrvu3TvcOJwLXWYmDBkC990HlSt7TyYXswIThap+ENz8FTgFQESOj2dQJeXDD+36iCOsbplzZdb8+farafZsOP98GDoUDj007Khciog24K480Bur8fSxqi4Uke7An4EqQOvEhFh03bvbdXaZfOfKrPR0WLPGJovv2dNLFLhCidZGMQK4GqgFPCUi/wEeAx5R1ZiShIh0EZFlIrJcRO7OZ53eIrJYRBaJyOuFfQP5+emnnNs+oNSVSV98kTOVY3YRv169PEm4Qot26ikNaKmqe0SkMrAOOFJVN8ay4eCIZChwBpAOzBSRsaq6OGKdxsCfgONVdZOI/KGobyS3Tz+16+yusc6VGdu2WRfXp5+GI4+0xupKlaBq1bAjcykq2hHFLlXdA6CqGcCKWJNEoB2wXFVXqOouYDSQu6XgGmCoqm4KXmd9IbYfVfYPqWOOKaktOpcCJkywAUNPP23dXb2InysB0Y4ojhGR+cFtAY4M7gugqtqygG3XBdZE3E8H2udapwmAiEzDCg3er6of596QiAwABgA0aNCggJc1ixdD+/Zw4okxre5c6luzBs4+244ipk6FE04IOyJXSkRLFImYc2I/oDHQGagHTBWRY1V1c+RKqjoMGAaQlpamsWx461b/nLgyYvZsaNsW6teHjz6yX0eVKxf8POdilO+pJ1VdFe0Sw7bXAvUj7tcLHouUDoxV1d2quhL4BkscxbJ8uc0/kT0HhXOl0rp1cOGFkJZmZcABzjjDk4QrcfGspToTaCwijUSkItAHGJtrnXexowlEpDZ2KmpFcV945Uq7btOmuFtyLgmpwssvQ7NmVgb8H//wIn4urmIZmV0kqpopIjcC47H2h5dUdZGIDAZmqerYYNmZIrIYyALuLGSDeZ6yKyM3a1bcLTmXhPr0sVLgxx8PL77oPTZc3Ilqwaf8RaQK0EBVl8U/pOjS0tJ01qxZ+S5fsiQnQWTXPXMu5UUW8Xv5ZWuEGzjQJ1hxMROR2aqaVpTnFvhfJiLnAHOBj4P7rUQk9ymkpPHHP9r10097knClxNKlNmp0xAi7f8UVcOONniRcwsTyn3Y/NiZiM4CqzsXmpkhK1YNJW7PnyHYuZe3ebe0Pxx1n/b2rVQs7IldGxdJGsVtVf5W9f57H1EU1LI0b+9GES3Fz59qI6rlzrezG00/DIYeEHZUro2JJFItE5GKgfFBy42bgi/iGVXRvv21zwjuX0tats8tbb8EFF4QdjSvjYjn1dBM2X/ZO4HWs3PitcYypyObOhZ07ff4Vl6I+/xyefdZud+kC333nScIlhVgSxTGqeo+q/l9wuTeo/ZR0xo+363ffDTUM5wpn61ZrnD7xRHjiCfu1AzZ/r3NJIJZE8S8RWSIifxORFnGPqBiyf4x5t3KXMsaPtyJ+zz4Lt9ziRfxcUiowUajqKdjMdhuAF0RkgYjcG/fICumTT2D16rCjcK4Q1qyx2bX2399OOz3xhPdsckkppo7YqrpOVZ8CrsPGVPw1nkEVxcZgPPe0ad7jySUxVZgxw27Xrw/jxsGcOV6CwyW1WAbcNRWR+0VkAfA01uMp6foV/etfdl2zZrhxOJevH3+0aUjbt88p4nf66V7EzyW9WLrHvgT8FzhLVX+IczxFNnOmXR91VLhxOLcPVRg5Em6/HTIy4OGHrU6TcymiwEShqh0TEUhxLF9u1+edBxUqhBqKc/vq3RvGjLFeTS++CE2ahB2Rc4WSb6IQkTdUtXdwyilyJHasM9wlzLZtdn3hheHG4dzvsrKssaxcOTjnHDj1VLj2Wq/P5FJStCOKW4Lr7okIpCR4t3OXFJYsgf79rQTHNdfA5ZeHHZFzxRJthrsfg5sD85jdbmBiwotNDJXSnYu/3bvhwQehVStYtgxq1Ag7IudKRCzHwWfk8VjXkg6kOLIbsmvXDjcOV4bNmWNTkv7lL3D++XZU0bt32FE5VyKitVFcjx05HCEi8yMWVQemxTuwwsjuGtuuXbhxuDLsp5/g55+tfkyPHmFH41yJitZG8TowDngIuDvi8a2q+ktcoyqkQw+1QpsVK4YdiStTpk6FBQts8pMuXaz7XZUqYUflXImLdupJVfV74AZga8QFETko/qEVTuvWYUfgyowtW2wa0pNPhqeeyini50nClVIFHVF0B2Zj3WMjC2MocEQc43IuOX30kXVz/eEHG0A3eLAX8XOlXr6JQlW7B9dJO+0p2Jzzy5ZB27ZhR+JKvTVrrP3h6KNtAF379mFH5FxCxFLr6XgRqRrcvlREhohIg/iHFpvt2619omPSjx93KUkVpk+32/Xrw4QJVgrck4QrQ2LpHvscsF1EjgMGAd8Br8Y1qkKYOtWu9+wJNw5XCv3wg9WF6dgxp4jfKad4rwlX5sSSKDJVVYEewDOqOhTrIpsUstsRzz473DhcKaJqNZmaNbMjiMce8yJ+rkyLpXrsVhH5E3AZcKKIlAOSrvSez5PtSkyvXvD229ar6cUXvSSxK/NiOaK4CNgJXKWq67C5KB6Na1SF8OGHdu2TFbliycrKOX953nnw/PMwcaInCeeIbSrUdcBrQA0R6Q5kqOorcY8sRtlHEs2ahRuHS2ELF9qppREj7P5ll3mlV+cixNLrqTcwA7gQ6A18JSK94h1YYRxyCOwXy0k05yLt2gUPPABt2sB33/n0iM7lI5av13uA/1PV9QAiUgf4BBgTz8Cci6vZs6FfPzuauPhieOIJqFMn7KicS0qxJIpy2UkisJHY2jacS14bN8LmzfD++9A9ZaZccS4UsSSKj0VkPDAquH8R8FH8QnIuTiZNsiJ+N98MZ54J334LlSuHHZVzSS+Wxuw7gReAlsFlmKreFe/AYvX1194+4Qrw66/WOH3qqfDcczmDbzxJOBeTaPNRNAYeA44EFgB3qOraRAUWi88+g1mzfLCdi+L99+G666zOyx13WOO1F/FzrlCiHVG8BHwA9MQqyD6dkIgKIXtmu0ceCTcOl6TWrIGePaFWLavX9OijPrG6c0UQ7aRNdVUdHtxeJiJfJyKgoqhXL+wIXNJQhS+/hE6dcor4derk9ZmcK4ZoRxSVRaS1iLQRkTZAlVz3CyQiXURkmYgsF5G7o6zXU0RURNIKE7xqYdZ2pV56Opx7rg2eyy7i17mzJwnniinaEcWPwJCI++si7itwarQNi0h5YChwBpAOzBSRsaq6ONd61YFbgK8KFzrMnWuDZyskXeUpl1B79sDw4XDnnZCZCUOGwAknhB2Vc6VGtImLTinmttsBy1V1BYCIjMYq0C7Otd7fgIeBOwv7AsuWwbHH+gyUZV7PnvDuu9arafhwOMInX3SuJMVz4FxdYE3E/fTgsd8Fp7Dqq+qH0TYkIgNEZJaIzNqwYQNgRxMzZ8Ill5Rs0C5FZGbmFPHr2dMSxCefeJJwLg5CG2EdlCsfgk2GFJWqDlPVNFVNqxOUWfjhB1t28slxDNIlp/nzbTKh4UFfi0svhauv9hLCzsVJPBPFWqB+xP16wWPZqgMtgMki8j3QARhb2AZtV4bs3An33WcTpK9a5bWZnEuQWKrHSjBX9l+D+w1EpF0M254JNBaRRiJSEegDjM1eqKq/qmptVT1cVQ8HpgPnquqsIr0TV7rNnGlVXgcPhr59YckSuOCCsKNyrkyI5YjiWaAj0De4vxXrzRSVqmYCNwLjgSXAG6q6SEQGi8i5RYz3d8OGFXcLLqVs2gTbtsFHH8Err9ggOudcQsRSJam9qrYRkTkAqropOEIokKp+RK4Cgqr613zW7RzLNrMtWGDXjRsX5lkupUycaH/oW26xIn7ffOPlN5wLQSxHFLuDMREKv89HsSeuUcWgXDmbRsDnmimFNm+Ga66B006DF17IKeLnScK5UMSSKJ4C3gH+ICJ/Bz4H/hHXqFzZ9d57Nq/tSy/BH/9oEwx5gnAuVAWeelLV10RkNnAaIMB5qrok7pEVwMt3lEKrV8OFF0LTpjB2LKR5BzjnkkEsvZ4aANuB97FeS78Fj4Vm40b4/ntoEGoUrkSoWr14sD/oJ59YDydPEs4ljVgasz/E2icEqAw0ApYBzeMYV1SbN0NWlv3wdCls9WqbK2LcOJg82UZPnnRS2FE553KJ5dTTsZH3g7IbA+MWUSH4QNwUtWcPPP883HWXHVE89ZQX8XMuiRV6ElFV/VpE2scjGFdGXHCBNVqfcYYNiDn88LAjcs5FUWCiEJHbI+6WA9oAP8QtIlc6ZWZan+Zy5eCii6BHD+jXzw8LnUsBsXSPrR5xqYS1WfSIZ1CulJk3D9q3zxlO37cvXHmlJwnnUkTUI4pgoF11Vb0jQfG40iQjAx58EB5+GA46CA45JOyInHNFkG+iEJH9VDVTRI5PZECulJgxA664ApYuteshQyxZOOdSTrQjihlYe8RcERkLvAn8lr1QVd+Oc2wulW3ZAjt2wMcfw1lnhR2Nc64YYun1VBnYiM2RnT2eQgFPFG5vEybAokVw221w+uk2V62X33Au5UVLFH8IejwtJCdBZPMCGi7Hpk1w++0wciQ0bw4DB1qC8CThXKkQrddTeaBacKkecTv74hy8/bYV8Xv1VfjTn2DWLE8QzpUy0Y4oflTVwQmLxKWe1auhTx9o0cImFGrdOuyInHNxEO2Iwju5u32pwpQpdrtBA5tc6KuvPEk4V4pFSxSnJSwKlxpWrYKuXaFz55xkccIJUKFCqGE55+Ir30Shqr8kMhCXxPbsgWeesYbqzz+Hp5+GE08MOyrnXIIUuiigK4POOw/ef9/GQ7zwAjRsGHZEzrkE8kTh8rZ7N5Qvb0X8+vaFXr3gssu8PpNzZVAsRQFdWfP119Cunc0ZAZYoLr/ck4RzZZQnCpdjxw4bC9GuHaxbB/Xrhx2Rcy4J+KknZ6ZPt+J933wDV10Fjz0GNWuGHZVzLgl4onDmt9+sXeJ//7M6Tc45F0jJRPH992FHUEp8/LEV8Rs0CE47zUqCV6wYdlTOuSSTkm0Uzz5r1y1ahBtHytq40U4zde0KL78Mu3bZ454knHN5SMlEsXUrNG7sVSMKTRXGjLEifq+/DvfeCzNneoJwzkWVkqeepk+Hiy4KO4oUtHo1XHwxtGxpc0ccd1zYETnnUkBKHlFs3w4HHxx2FClC1Qr3gY2onjzZMq0nCedcjFIyUbgYrVwJZ55pDdXZRfw6dYL9UvJA0jkXEk8UpVFWFjz5pLX2f/UVPPecF/FzzhWZ/7QsjXr0gA8/hG7drAyHj7B2zhWDJ4rSIrKI32WXWX2miy/2+kzOuWKL66knEekiIstEZLmI3J3H8ttFZLGIzBeRT0WkwPrVWVl28WmZI8yaBWlpdooJrEvYJZd4knDOlYi4JQoRKQ8MBboCzYC+ItIs12pzgDRVbQmMAR4paLvbt9t1hw4lGW2K2rED7roL2reHDRt8ngjnXFzE84iiHbBcVVeo6i5gNNAjcgVVnaSqwVc/04F6BW1U1a6rVCnRWFPPl19aF9dHHrEifosXQ/fuYUflnCuF4tlGURdYE3E/HWgfZf3+wLi8FojIAGAAQI0ax5RUfKltxw6bovSTT6z7q3POxUlSdI8VkUuBNODRvJar6jBVTVPVtMqVqwLQtGkCA0wWH30Ejwa76NRTYckSTxLOubiLZ6JYC0T2y6wXPLYXETkduAc4V1V3FrTRrCyoVg1q1SqxOJPfzz/DpZfC2WfDa6/lFPGrUCHcuJxzZUI8E8VMoLGINBKRikAfYGzkCiLSGngBSxLrY9nob79B27YlHmtyUoXRo+3w6Y034L77YMYML+LnnEuouLVRqGqmiNwIjAfKAy+p6iIRGQzMUtWx2KmmasCbYl05V6vqudG3C7VrxyvqJLN6tZUDP+44GDECjj027Iicc2VQXAfcqepHwEe5HvtrxG2fSi03Vfj0U5tlrmFDq9H0f/9ng+mccy4ESdGY7QLffWeN02eckVPEr0MHTxLOuVB5okgGWVkwZIidWpo9G154wYv4OeeShtd6SgbnnAPjxtmAueeeg3oFjjt0zrmE8UQRll27bF6IcuWgXz8r5Nenj9dncs4lHT/1FIYZM6yP77PP2v3eva3aqycJ51wS8kSRSNu3w6BB0LEjbNoERx4ZdkTOOVcgP/WUKJ9/bmMiVqyAa6+Fhx+GGjXCjso55wrkiSJRsicWmjQJOncOOxrnnIuZJ4p4ev99K9z3xz/CKadYKfD9fJc751KLt1HEw4YNNg3puefCqFE5Rfw8STjnUpAnipKkCq+/bkX8xoyBwYPhq6+8iJ9zLqX5T9yStHo1XHkltG5tRfyaNw87IuecKzY/oiiuPXtg/Hi73bAhfPYZTJvmScI5V2p4oiiOb7+1mea6dIGpU+2xdu28iJ9zrlTxRFEUmZk2JWnLljB3rp1m8iJ+zrlSytsoiqJ7dzvd1KOHleE47LCwI3IuKe3evZv09HQyMjLCDqXMqFy5MvXq1aNCCU6V7IkiVjt32hzV5crB1VfDVVfBhRd6fSbnokhPT6d69eocfvjhiH9W4k5V2bhxI+np6TRq1KjEtuunnmIxfTq0aQNDh9r9Xr2skJ//4zsXVUZGBrVq1fIkkSAiQq1atUr8CM4TRTS//Qa33QadOsHWrdC4cdgROZdyPEkkVjz2t596ys9nn1kRv5UrYeBAeOghOOCAsKNyzrmE8yOK/GRmWpvElCl2ysmThHMp691330VEWLp06e+PTZ48me7du++1Xr9+/RgzZgxgDfF33303jRs3pk2bNnTs2JFx48YVO5aHHnqIo446iqOPPprx2WOwcpk4cSJt2rShRYsWXHHFFWRmZgLw2muv0bJlS4499lg6derEvHnzih1PLDxRRHr3XTtyACvit2gRnHRSqCE554pv1KhRnHDCCYwaNSrm5/zlL3/hxx9/ZOHChXz99de8++67bN26tVhxLF68mNGjR7No0SI+/vhjBg4cSFZW1l7r7NmzhyuuuILRo0ezcOFCGjZsyMsvvwxAo0aNmDJlCgsWLOAvf/kLAwYMKFY8sfJTTwA//QQ33QRvvmmN1oMGWX0mL+LnXIm59VYbdlSSWrWCJ56Ivs62bdv4/PPPmTRpEueccw4PPPBAgdvdvn07w4cPZ+XKlVSqVAmAgw8+mN69excr3vfee48+ffpQqVIlGjVqxFFHHcWMGTPo2LHj7+ts3LiRihUr0qRJEwDOOOMMHnroIfr370+nTp1+X69Dhw6kp6cXK55Yle0jClV49VVo1gzeew/+/nfr4eRF/JwrNd577z26dOlCkyZNqFWrFrNnzy7wOcuXL6dBgwYcEMMp59tuu41WrVrtc/nnP/+5z7pr166lfv36v9+vV68ea9eu3Wud2rVrk5mZyaxZswAYM2YMa9as2WdbI0aMoGvXrgXGVxJS7ifz7t0l+D2+erWNiUhLs9HVxxxTQht2zuVW0C//eBk1ahS33HILAH369GHUqFG0bds2395Bhe019Pjjjxc7xtyvP3r0aG677TZ27tzJmWeeSflcZYEmTZrEiBEj+Pzzz0v0tfOTcokiK8umnC6y7CJ+XbtaEb9p06zaq9dncq7U+eWXX5g4cSILFixARMjKykJEePTRR6lVqxabNm3aZ/3atWtz1FFHsXr1arZs2VLgUcVtt93GpEmT9nm8T58+3H333Xs9Vrdu3b2ODtLT06lbt+4+z+3YsSOfffYZABMmTOCbb775fdn8+fO5+uqrGTduHLVq1Sp4J5QEVU2pC7TV4cO1aJYtUz3xRFVQnTy5iBtxzsVq8eLFob7+Cy+8oAMGDNjrsZNOOkmnTJmiGRkZevjhh/8e4/fff68NGjTQzZs3q6rqnXfeqf369dOdO3eqqur69ev1jTfeKFY8Cxcu1JYtW2pGRoauWLFCGzVqpJmZmfus99NPP6mqakZGhp566qn66aefqqrqqlWr9Mgjj9Rp06ZFfZ289jswS4v4vVs22igyM+Hhh62I34IF8O9/e28m58qAUaNGcf755+/1WM+ePRk1ahSVKlXiP//5D1deeSWtWrWiV69evPjii9SoUQOABx98kDp16tCsWTNatGhB9+7dY2qziKZ58+b07t2bZs2a0aVLF4YOHfr7aaVu3brxww8/APDoo4/StGlTWrZsyTnnnMOpp54KwODBg9m4cSMDBw6kVatWpKWlFSueWIklmtQhkqbDh8/i6qsL8aSzzoIJE+CCC2xMxCGHxC0+51yOJUuW0LRp07DDKHPy2u8iMltVi5RZUq6NImYZGTZgrnx5GDDALj17hh2Vc86lnNJ56mnaNOtgnV3Er2dPTxLOOVdEpStRbNsGN99skwhlZIAf8joXulQ7vZ3q4rG/S0+imDIFWrSAZ56BG2+EhQvhjDPCjsq5Mq1y5cps3LjRk0WCaDAfReXKlUt0u6WrjWL//a3q6/HHhx2Jcw4beZyens6GDRvCDqXMyJ7hriSldqJ4+21YuhT+/Gc4+WTr+uoD55xLGhUqVCjRmdZcOOJ66klEuojIMhFZLiJ357G8koj8N1j+lYgcHst2q/y6zmaZ69kT3nkHdu2yBZ4knHOuxMUtUYhIeWAo0BVoBvQVkWa5VusPbFLVo4DHgYcL2m4tNtLrr03hgw+sJPgXX3gRP+eci6N4HlG0A5ar6gpV3QWMBnrkWqcH8HJwewxwmhRQkashq9jasAXMmwd3321jJZxzzsVNPNso6gKRtXHTgfb5raOqmSLyK1AL+DlyJREZAGTP0LGzzpLPF3qlVwBqk2tflWG+L3L4vsjh+yLH0UV9Yko0ZqvqMGAYgIjMKuow9NLG90UO3xc5fF/k8H2RQ0RmFfW58Tz1tBaoH3G/XvBYnuuIyH5ADWBjHGNyzjlXSPFMFDOBxiLSSEQqAn2AsbnWGQtcEdzuBUxUH5njnHNJJW6nnoI2hxuB8UB54CVVXSQig7G66GOBEcCrIrIc+AVLJgUZFq+YU5Dvixy+L3L4vsjh+yJHkfdFypUZd845l1ilp9aTc865uPBE4ZxzLqqkTRTxKv+RimLYF7eLyGIRmS8in4pIwzDiTISC9kXEej1FREWk1HaNjGVfiEjv4H9jkYi8nugYEyWGz0gDEZkkInOCz0m3MOKMNxF5SUTWi8jCfJaLiDwV7Kf5ItImpg0XdbLteF6wxu/vgCOAisA8oFmudQYCzwe3+wD/DTvuEPfFKcD+we3ry/K+CNarDkwFpgNpYccd4v9FY2AOUDO4/4ew4w5xXwwDrg9uNwO+DzvuOO2Lk4A2wMJ8lncDxgECdAC+imW7yXpEEZfyHymqwH2hqpNUdXtwdzo2ZqU0iuX/AuBvWN2wjEQGl2Cx7ItrgKGquglAVdcnOMZEiWVfKHBAcLsG8EMC40sYVZ2K9SDNTw/gFTXTgQNF5NCCtpusiSKv8h9181tHVTOB7PIfpU0s+yJSf+wXQ2lU4L4IDqXrq+qHiQwsBLH8XzQBmojINBGZLiJdEhZdYsWyL+4HLhWRdOAj4KbEhJZ0Cvt9AqRICQ8XGxG5FEgDTg47ljCISDlgCNAv5FCSxX7Y6afO2FHmVBE5VlU3hxlUSPoCI1X1XyLSERu/1UJV94QdWCpI1iMKL/+RI5Z9gYicDtwDnKuqOxMUW6IVtC+qAy2AySLyPXYOdmwpbdCO5f8iHRirqrtVdSXwDZY4SptY9kV/4A0AVf0SqIwVDCxrYvo+yS1ZE4WX/8hR4L4QkdbAC1iSKK3noaGAfaGqv6pqbVU9XFUPx9przlXVIhdDS2KxfEbexY4mEJHa2KmoFQmMMVFi2RergdMARKQplijK4vysY4HLg95PHYBfVfXHgp6UlKeeNH7lP1JOjPviUaAa8GbQnr9aVc8NLeg4iXFflAkx7ovxwJkishjIAu5U1VJ31B3jvhgEDBeR27CG7X6l8YeliIzCfhzUDtpj7gMqAKjq81j7TDdgObAduDKm7ZbCfeWcc64EJeupJ+ecc0nCE4VzzrmoPFE455yLyhOFc865qDxROOeci8oThUtKIpIlInMjLodHWXdbCbzeSBFZGbzW18Ho3cJu40URaRbc/nOuZV8UN8ZgO9n7ZaGIvC8iBxawfqvSWinVJY53j3VJSUS2qWq1kl43yjZGAh+o6hgRORN4TFVbFmN7xY6poO2KyMvAN6r69yjr98Mq6N5Y0rG4ssOPKFxKEJFqwVwbX4vIAhHZp2qsiBwqIlMjfnGfGDx+poh8GTz3TREp6At8KnBU8Nzbg20tFJFbg8eqisiHIjIvePyi4PHJIpImIv8EqgRxvBYs2xZcjxaRsyNiHikivUSkvIg8KiIzg3kCro1ht3xJUNBNRNoF73GOiHwhIkcHo5QHAxcFsVwUxP6SiMwI1s2r+q5zewu7frpf/JLXBRtJPDe4vINVETggWFYbG1mafUS8LbgeBNwT3C6P1X6qjX3xVw0evwv4ax6vNxLoFdy+EPgKaAssAKpiI98XAa2BnsDwiOfWCK4nE8x/kR1TxDrZMZ4PvBzcrohV8qwCDADuDR6vBMwCGuUR57aI9/cm0CW4fwCwX3D7dOCt4HY/4JmI5/8DuDS4fSBW/6lq2H9vvyT3JSlLeDgH7FDVVtl3RKQC8A8ROQnYg/2SPhhYF/GcmcBLwbrvqupcETkZm6hmWlDepCL2Szwvj4rIvVgNoP5YbaB3VPW3IIa3gROBj4F/icjD2OmqzwrxvsYBT4pIJaALMFVVdwSnu1qKSK9gvRpYAb+VuZ5fRUTmBu9/CfC/iPVfFpHGWImKCvm8/pnAuSJyR3C/MtAg2JZzefJE4VLFJUAdoK2q7harDls5cgVVnRokkrOBkSIyBNgE/E9V+8bwGneq6pjsOyJyWl4rqeo3YvNedAMeFJFPVXVwLG9CVTNEZDJwFnARNskO2IxjN6nq+AI2sUNVW4nI/lhtoxuAp7DJmiap6vlBw//kfJ4vQE9VXRZLvM6Bt1G41FEDWB8kiVOAfeYFF5sr/CdVHQ68iE0JOR04XkSy2xyqikiTGF/zM+A8EdlfRKpip40+E5HDgO2q+h+sIGNe8w7vDo5s8vJfrBhb9tEJ2Jf+9dnPEZEmwWvmSW1Gw5uBQZJTZj+7XHS/iFW3Yqfgso0HbpLg8Eqs8rBzUXmicKniNSBNRBYAlwNL81inMzBPROZgv9afVNUN2BfnKBGZj512OiaWF1TVr7G2ixlYm8WLqjoHOBaYEZwCug94MI+nDwPmZzdm5zIBm1zqE7WpO8ES22LgaxFZiJWNj3rEH8QyH5uU5xHgoeC9Rz5vEtAsuzEbO/KoEMS2KLjvXFTePdY551xUfkThnHMuKk8UzjnnovJE4ZxzLipPFM4556LyROGccy4qTxTOOeei8kThnHMuqv8HsoGL+PwGKn8AAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/test-data/test-data.csv')\ntest_data.columns = ['Comment', 'Majority_Label']\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:20:02.277130Z","iopub.execute_input":"2022-12-12T04:20:02.278261Z","iopub.status.idle":"2022-12-12T04:20:02.315364Z","shell.execute_reply.started":"2022-12-12T04:20:02.278181Z","shell.execute_reply":"2022-12-12T04:20:02.314174Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                             Comment  Majority_Label\n0  بالله عليك ريحي جمالك\\nوخطي الخوض في الشرع\\nيا...             NaN\n1  لو تريدين اخذ حقوق المرأة خوذيهم بطريقة الصح م...             NaN\n2                ناقصة عقل ودين أنت يا صاحبة المنشور             NaN\n3  اي عمل ممكن للمرأة اتقانه بأكثر كفاءة و دقة و ...             NaN\n4  الشي الوحيد الذي تمهر فيه المرٱه عن الرجل هو ا...             NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Majority_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>بالله عليك ريحي جمالك\\nوخطي الخوض في الشرع\\nيا...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>لو تريدين اخذ حقوق المرأة خوذيهم بطريقة الصح م...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ناقصة عقل ودين أنت يا صاحبة المنشور</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اي عمل ممكن للمرأة اتقانه بأكثر كفاءة و دقة و ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>الشي الوحيد الذي تمهر فيه المرٱه عن الرجل هو ا...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.Comment = test_data.Comment.apply(cleaning_content)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:20:06.284127Z","iopub.execute_input":"2022-12-12T04:20:06.284904Z","iopub.status.idle":"2022-12-12T04:20:06.293934Z","shell.execute_reply.started":"2022-12-12T04:20:06.284865Z","shell.execute_reply":"2022-12-12T04:20:06.293013Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#model = model.to(device)\nmodel = bert_classifier   \ndef predict(review_text):\n    encoded_review = tokenizer.encode_plus(\n    review_text,\n    max_length=MAX_LEN,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    padding='longest',\n    return_attention_mask=True,\n    return_tensors='pt',\n    )\n\n    input_ids = encoded_review['input_ids'].to(device)\n    attention_mask = encoded_review['attention_mask'].to(device)\n    output = model(input_ids, attention_mask)\n    _, prediction = torch.max(output, dim=1)\n    #print(f'Review text: {review_text}')\n    index = output.cpu().data.numpy().argmax()\n    #print(f'Sentiment  : {index}')\n    return index\n\n\n\nresults = test_data.apply(lambda l: predict(l.Comment), axis=1)\n\ntest_data.Majority_Label = results\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:21:34.486471Z","iopub.execute_input":"2022-12-12T04:21:34.486895Z","iopub.status.idle":"2022-12-12T04:21:35.042203Z","shell.execute_reply.started":"2022-12-12T04:21:34.486862Z","shell.execute_reply":"2022-12-12T04:21:35.041268Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:37:30.199423Z","iopub.execute_input":"2022-12-12T04:37:30.200190Z","iopub.status.idle":"2022-12-12T04:37:30.212294Z","shell.execute_reply.started":"2022-12-12T04:37:30.200150Z","shell.execute_reply":"2022-12-12T04:37:30.211018Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                                              Comment  Majority_Label\n0   بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...               0\n1   تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...               1\n2                        ناقصه عقل ودين صاحبه المنشور               1\n3   اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...               0\n4   الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...               0\n5   العجيب امركم يتها النساء لماذا تكروهن جنسكم وت...               0\n6   فعلا الرجل عاله وقت توا نساء العالم الناجحات ت...               1\n7   الباطل بعينه المراه تستطيع العيش بدون رجل يعني...               1\n8                         مافيش شي الدنيا احلي النساء               0\n9                         مجرد ببغاوات تنقل وتقلد فقط               0\n10                      الامهات يمارسن ال اكثر الاباء               0\n11  بجد النسويات ناس مري بصحيح الحمدلله عافاني ابت...               0\n12  بالله انت عقلك معاك ماجرتيه ثور قليله الادب هد...               1\n13                        المراه فالمطبخ ارضو بالواقع               0\n14         اعرفي تكلمي روحي احسن ليك ولينا ليبيا رجال               0\n15  تخيلوا ستترشح للبرلمان غيرها الكثير فازت بالاص...               0\n16  متعرفش تقول كلمتين بعضهم بري شدي مكانك وقعمزي خير               0\n17  حتي القرايه عندها بالهون الاخت بري سلم بنتي شد...               0\n18                انتي ماقدرتيش تلمي شعرك بتلمي الشعب               1\n19       مش هب ودب راهو صحباتك عطوك الحافز راهم يكدبو               0\n20          مش عارفه تدوي كلمتين ومرشحه روحك شن بنفهم               0\n21  انا عنصر المراه شي الا انها تمسك ليبيا ليبيا ي...               0\n22  بدل ماترشح روحها تمشي دور مدرسه تقرا فاها وتتع...               0\n23  بري كملي نومك خيرلك ناقصنا انتي وفوق تنكل شن ب...               0\n24        ادا الكلام فضه اوقات يكون سكوت واحديسكت خير               0\n25                         مءهله نهاءيااااا النواااحي               0\n26  شعرها مش قادره تلمه تبي تلم الشمل وتشد ليبيا و...               0\n27  المجد للنساء اللبيبات الذكر الليبي قتل ومعنف و...               1\n28  ياروحي هاربه بلادها لبلادات ناس يبو امان حسبي ...               1\n29  رجال قوامون علي نساء ورجال اعلي درجه الله كرم ...               0\n30                    لعنه الله ياكافره الي يوم الدين               1\n31       سواء راجل او مراه الواحد مايتمناش الشر للناس               1\n32  العلم انه معظمهم ينطبقش عليهم مصطلح وليه ينطبق...               0\n33  النوعيه هادي تلقاها خاشه لجمعيه باش تحصل سياره...               1\n34    يديرو روحهم مثقفات النوع وهما اخرهم شهاده متوسط               1\n35  نحترم الانثي القويه نكنلها الاحترام حقكم المسا...               0\n36  يدورو العدل المساواه ابسط مثال الخدمه طلبات ال...               0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Majority_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>بالله ريحي جمالك وخطي الخوض الشرع جو يهله واله...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>تريدين اخذ حقوق المراه خوذيهم بطريقه الصح مش ب...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ناقصه عقل ودين صاحبه المنشور</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اي عمل ممكن للمراه اتقانه باكثر كفاءه دقه جوده...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>الشي الوحيد تمهر المرٱه الرجل العمل الكوجينه ت...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>العجيب امركم يتها النساء لماذا تكروهن جنسكم وت...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>فعلا الرجل عاله وقت توا نساء العالم الناجحات ت...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>الباطل بعينه المراه تستطيع العيش بدون رجل يعني...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>مافيش شي الدنيا احلي النساء</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>مجرد ببغاوات تنقل وتقلد فقط</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>الامهات يمارسن ال اكثر الاباء</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>بجد النسويات ناس مري بصحيح الحمدلله عافاني ابت...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>بالله انت عقلك معاك ماجرتيه ثور قليله الادب هد...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>المراه فالمطبخ ارضو بالواقع</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>اعرفي تكلمي روحي احسن ليك ولينا ليبيا رجال</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>تخيلوا ستترشح للبرلمان غيرها الكثير فازت بالاص...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>متعرفش تقول كلمتين بعضهم بري شدي مكانك وقعمزي خير</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>حتي القرايه عندها بالهون الاخت بري سلم بنتي شد...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>انتي ماقدرتيش تلمي شعرك بتلمي الشعب</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>مش هب ودب راهو صحباتك عطوك الحافز راهم يكدبو</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>مش عارفه تدوي كلمتين ومرشحه روحك شن بنفهم</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>انا عنصر المراه شي الا انها تمسك ليبيا ليبيا ي...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>بدل ماترشح روحها تمشي دور مدرسه تقرا فاها وتتع...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>بري كملي نومك خيرلك ناقصنا انتي وفوق تنكل شن ب...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>ادا الكلام فضه اوقات يكون سكوت واحديسكت خير</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>مءهله نهاءيااااا النواااحي</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>شعرها مش قادره تلمه تبي تلم الشمل وتشد ليبيا و...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>المجد للنساء اللبيبات الذكر الليبي قتل ومعنف و...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>ياروحي هاربه بلادها لبلادات ناس يبو امان حسبي ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>رجال قوامون علي نساء ورجال اعلي درجه الله كرم ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>لعنه الله ياكافره الي يوم الدين</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>سواء راجل او مراه الواحد مايتمناش الشر للناس</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>العلم انه معظمهم ينطبقش عليهم مصطلح وليه ينطبق...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>النوعيه هادي تلقاها خاشه لجمعيه باش تحصل سياره...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>يديرو روحهم مثقفات النوع وهما اخرهم شهاده متوسط</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>نحترم الانثي القويه نكنلها الاحترام حقكم المسا...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>يدورو العدل المساواه ابسط مثال الخدمه طلبات ال...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.to_csv('/kaggle/working/test_result.csv', encoding='utf-16', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:37:51.919812Z","iopub.execute_input":"2022-12-12T04:37:51.920200Z","iopub.status.idle":"2022-12-12T04:37:51.933798Z","shell.execute_reply.started":"2022-12-12T04:37:51.920167Z","shell.execute_reply":"2022-12-12T04:37:51.932743Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}